title,authors,year,citations,abstract,result_id,publication_info,pdf_links,url,total_results,search_time
Synthesizer: Rethinking self-attention for transformer models,"Y Tay, D Bahri, D Metzler, DC Juan…",2021.0,490,"… SYNTHESIZER model. At its core, our model is essentially a Transformer model with self-attention … Figure 3.1 illustrates the key ideas behind (a) Transformer (b) Dense Synthesizers …",d2tbBUAhy7kJ,"Y Tay, D Bahri, D Metzler, DC Juan… - International …, 2021 - proceedings.mlr.press",[],https://proceedings.mlr.press/v139/tay21a.html,unknown,unknown
Rethinking transformers for efficiency and scalability,"H Jiahao, Y Bao",,3,… two primary subcomponents: multi-head self-attention and feedforward neural networks [24]. Multi-Head Self-Attention The self-attention mechanism computes a weighted sum of all …,PEPGUTiBbNUJ,"H Jiahao, Y Bao - Available at SSRN 5161897, 2025 - papers.ssrn.com",[],https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5161897,unknown,unknown
Rethinking Spiking Self-Attention Mechanism: Implementing a-XNOR Similarity Calculation in Spiking Transformers,"Y Xiao, S Wang, D Zhang, W Wei…",2025.0,9,… the spiking self-attention mechanism and and develop an αXNOR Spiking Self-Attention (α-… significantly improves the performance of spiking Transformers across various architectures. …,IEwv1UWu-aMJ,"Y Xiao, S Wang, D Zhang, W Wei… - Proceedings of the …, 2025 - openaccess.thecvf.com",[],https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking_CVPR_2025_paper.html,unknown,unknown
Synthesizer Based Efficient Self-Attention for Vision Tasks,"G Zhu, J Zhang, Y Feng, H Lan",,0,"… is applied as the core mechanism of the neural network named transformer, which is widely … self-attention. Inspired by the NLP-like synthesizer[16,19], we propose a plug-in synthesizer …",AKsOxYOey7UJ,"G Zhu, J Zhang, Y Feng, H Lan - arXiv preprint arXiv:2201.01410, 2022 - arxiv.org",[],https://arxiv.org/abs/2201.01410,unknown,unknown
Et: re-thinking self-attention for transformer models on gpus,"S Chen, S Huang, S Pandey, B Li, GR Gao…",2021.0,47,"… To this end, we introduce ET that rE-thinks self-attention computation for Transformer … a novel self-attention architecture, which encompasses two tailored self-attention operators with …",RvyP3OC12-0J,"S Chen, S Huang, S Pandey, B Li, GR Gao… - Proceedings of the …, 2021 - dl.acm.org",[],https://dl.acm.org/doi/abs/10.1145/3458817.3476138,unknown,unknown
Mixsynthformer: A transformer encoder-like structure with mixed synthetic self-attention for efficient human pose estimation,"Y Sun, AW Dougherty, Z Zhang…",2023.0,13,"… transformer encoder, except for replacing the self-attention … Attention layers in transformers need position embeddings to … Synthesizer: Rethinking self-attention for transformer models. …",8cqcaEX7iaEJ,"Y Sun, AW Dougherty, Z Zhang… - Proceedings of the …, 2023 - openaccess.thecvf.com",[],http://openaccess.thecvf.com/content/ICCV2023/html/Sun_MixSynthFormer_A_Transformer_Encoder-like_Structure_with_Mixed_Synthetic_Self-attention_for_ICCV_2023_paper.html,unknown,unknown
Transformer-based end-to-end speech recognition with local dense synthesizer attention,"M Xu, S Li, XL Zhang",2021.0,47,"… introduce the classic dotproduct self-attention and its variant—… -Transformer significantly outperforms the DSATransformer, and achieves a slightly lower CER than the SA-Transformer, …",I6vijscPDeoJ,"M Xu, S Li, XL Zhang - ICASSP 2021-2021 IEEE International …, 2021 - ieeexplore.ieee.org",[],https://ieeexplore.ieee.org/abstract/document/9414353/,unknown,unknown
Rethinking attention with performers,"K Choromanski, V Likhosherstov, D Dohan…",2020.0,2744,"… Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers … is applied in encoder self-attention and encoder-…",jcqZRoSMA3UJ,"K Choromanski, V Likhosherstov, D Dohan… - arXiv preprint arXiv …, 2020 - arxiv.org",[],https://arxiv.org/abs/2009.14794,unknown,unknown
Refiner: Refining self-attention for vision transformers,"D Zhou, Y Shi, B Kang, W Yu, Z Jiang, Y Li…",2021.0,98,"… We believe it is inspiring for rethinking the common random cropping techniques. More importantly, it motivates future studies to dig into the effect of the model’s receptive field in …",ZYIefC4_aXIJ,"D Zhou, Y Shi, B Kang, W Yu, Z Jiang, Y Li… - arXiv preprint arXiv …, 2021 - arxiv.org",[],https://arxiv.org/abs/2106.03714,unknown,unknown
Structural attention: Rethinking transformer for unpaired medical image synthesis,"VMH Phan, Y Xie, B Zhang, Y Qi, Z Liao…",2024.0,28,"… Transformer-based models excel in … , Transformer can converge to non-optimal solutions in the absence of paired data. To address this, we introduce UNet Structured Transformer (…",cqYFJbIBtWIJ,"VMH Phan, Y Xie, B Zhang, Y Qi, Z Liao… - … Conference on Medical …, 2024 - Springer",[],https://link.springer.com/chapter/10.1007/978-3-031-72104-5_66,unknown,unknown
