# 正交随机注意力实验训练配置方案

## 概述

本文档为"正交随机注意力"(Orthogonal Random Attention) Transformer模型提供完整的训练配置方案，涵盖Small、Base、Large三种模型规模。

---

## 1. 模型配置

### 1.1 模型架构参数

| 参数 | Small | Base | Large |
|------|-------|------|-------|
| **层数** | 6 | 12 | 24 |
| **隐藏维度** | 512 | 768 | 1024 |
| **注意力头数** | 8 | 12 | 16 |
| **每头维度** | 64 | 64 | 64 |
| **FFN维度** | 2048 | 3072 | 4096 |
| **Dropout率** | 0.1 | 0.1 | 0.1 |
| **注意力Dropout** | 0.1 | 0.1 | 0.1 |
| **最大序列长度** | 512 | 512 | 512 |
| **词表大小** | 32000 | 32000 | 32000 |
| **参数量** | 30.6M | 88.3M | 259.4M |

### 1.2 正交随机注意力特性

- **Q投影**: 可学习的线性投影 (d_model x d_model)
- **K,V投影**: 使用正交随机矩阵（无需学习参数）
- **正交矩阵生成**: 基于Haar测度的随机正交矩阵
- **参数节省**: 相比标准注意力减少约30-40%的可训练参数

### 1.3 配置选择理由

**Small规模 (30.6M参数)**
- 适用于快速原型验证和消融实验
- 可在单卡消费级GPU上训练
- 训练周期短，便于快速迭代

**Base规模 (88.3M参数)**
- 对标BERT-base和GPT-medium
- 平衡性能和计算成本
- 适合中等规模数据集训练

**Large规模 (259.4M参数)**
- 对标BERT-large和GPT-large
- 需要多卡并行训练
- 适合大规模预训练任务

---

## 2. 优化器配置

### 2.1 AdamW参数

| 参数 | Small | Base | Large |
|------|-------|------|-------|
| **优化器** | AdamW | AdamW | AdamW |
| **学习率** | 5e-4 | 3e-4 | 2e-4 |
| **Betas** | (0.9, 0.98) | (0.9, 0.98) | (0.9, 0.98) |
| **Epsilon** | 1e-8 | 1e-8 | 1e-8 |
| **权重衰减** | 0.01 | 0.01 | 0.01 |
| **梯度裁剪** | 1.0 | 1.0 | 1.0 |

### 2.2 学习率选择理由

**Small模型 (5e-4)**
- 正交随机注意力需要较高的初始学习率来快速学习Q投影矩阵
- 正交随机矩阵的稳定性允许使用更大学习率
- 5e-4是标准Transformer的1.5-2倍

**Base模型 (3e-4)**
- 使用标准Transformer学习率3e-4
- 正交随机矩阵的引入减少了需要学习的参数
- 可以使用稍大的学习率而不导致不稳定

**Large模型 (2e-4)**
- 需要更保守的学习率2e-4
- 由于参数量增加，使用较小的学习率保证训练稳定性
- 正交随机矩阵的正交性约束有助于防止梯度爆炸

### 2.3 其他优化器设置

```python
# 权重衰减参数分组
no_decay = ["bias", "LayerNorm.weight"]
optimizer_grouped_parameters = [
    {
        "params": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
        "weight_decay": 0.01
    },
    {
        "params": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
        "weight_decay": 0.0
    }
]
```

---

## 3. 学习率调度策略

### 3.1 调度配置

| 参数 | Small | Base | Large |
|------|-------|------|-------|
| **Warmup步数** | 4000 | 8000 | 12000 |
| **Warmup策略** | Linear | Linear | Linear |
| **衰减策略** | Cosine | Cosine with Restarts | Polynomial |
| **最小学习率比例** | 0.1 | 0.1 | 0.05 |
| **总训练步数** | 100000 | 300000 | 500000 |
| **重启间隔** | - | 100000 | - |
| **多项式幂次** | - | - | 2.0 |

### 3.2 调度策略说明

**Small模型 - Cosine衰减**
```python
lr = base_lr * (min_lr_ratio + (1 - min_lr_ratio) * 
                0.5 * (1 + cos(pi * current_step / total_steps)))
```
- 训练100K步，warmup 4K步
- 使用cosine衰减可以平滑降低学习率
- 避免训练后期的震荡

**Base模型 - Cosine with Restarts**
```python
# 每100K步重启一次
restart_cycle = 100000
cycle_position = current_step % restart_cycle
lr = base_lr * (min_lr_ratio + (1 - min_lr_ratio) * 
                0.5 * (1 + cos(pi * cycle_position / restart_cycle)))
```
- 训练300K步，warmup 8K步
- 使用cosine with restarts策略，每100K步重启一次
- 有助于跳出局部最优，特别适合注意力机制探索

**Large模型 - Polynomial衰减**
```python
progress = current_step / total_steps
lr = (base_lr - min_lr) * (1 - progress) ** power + min_lr
```
- 训练500K步，warmup 12K步
- 使用polynomial衰减(power=2)
- 前期学习率下降较慢，后期快速收敛
- 适合大规模模型的精细调优

---

## 4. 训练配置

### 4.1 基本训练参数

| 参数 | Small | Base | Large |
|------|-------|------|-------|
| **每GPU批次大小** | 64 | 32 | 16 |
| **梯度累积步数** | 1 | 2 | 4 |
| **有效批次大小** | 64 | 64 | 64 |
| **最大训练步数** | 100000 | 300000 | 500000 |
| **混合精度** | FP16 | FP16 | BF16 |
| **序列长度** | 512 | 512 | 512 |

### 4.2 检查点与日志

| 参数 | Small | Base | Large |
|------|-------|------|-------|
| **评估间隔** | 5000步 | 10000步 | 10000步 |
| **保存间隔** | 10000步 | 20000步 | 25000步 |
| **日志间隔** | 100步 | 100步 | 100步 |
| **数据加载 workers** | 4 | 8 | 8 |
| **Pin Memory** | True | True | True |

### 4.3 混合精度说明

**FP16 (Small, Base)**
- 使用PyTorch AMP自动混合精度
- 动态损失缩放 (初始值256，动态调整)
- 节省约40%显存，加速1.5-2倍

**BF16 (Large)**
- 使用BF16混合精度，比FP16更稳定
- 不需要损失缩放
- 动态范围更大，适合大规模训练
- 需要Ampere架构GPU (A100, RTX 30系列)

### 4.4 训练代码示例

```python
from torch.cuda.amp import autocast, GradScaler

# 初始化
scaler = GradScaler()

# 训练循环
for step, batch in enumerate(dataloader):
    # 前向传播
    with autocast(dtype=torch.float16):  # 或 bfloat16
        outputs = model(**batch)
        loss = outputs.loss / gradient_accumulation_steps
    
    # 反向传播
    scaler.scale(loss).backward()
    
    # 梯度更新
    if (step + 1) % gradient_accumulation_steps == 0:
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()
        optimizer.zero_grad()
```

---

## 5. 计算资源估算

### 5.1 显存需求分解

| 组件 | Small | Base | Large |
|------|-------|------|-------|
| **参数显存** | 0.06 GB | 0.16 GB | 0.48 GB |
| **梯度显存** | 0.06 GB | 0.16 GB | 0.48 GB |
| **优化器状态** | 0.23 GB | 0.66 GB | 1.93 GB |
| **激活值** | 0.94 GB | 0.70 GB | 0.47 GB |
| **正交矩阵缓存** | 0.001 GB | 0.001 GB | 0.002 GB |
| **总显存需求** | **1.54 GB** | **2.03 GB** | **4.04 GB** |

### 5.2 训练时间估算

| 模型 | 单卡A100 | 4卡A100 | 8卡A100 |
|------|----------|---------|---------|
| **Small** | 6.6小时 | 2.0小时 | - |
| **Base** | 16.7小时 | 4.2小时 | 2.1小时 |
| **Large** | 23.9小时 | 6.0小时 | 3.0小时 |

### 5.3 推荐GPU配置

| 模型 | 推荐配置 | 最低配置 |
|------|----------|----------|
| **Small** | 1x RTX 3090 / 1x A100 40GB | 1x RTX 2080Ti 11GB |
| **Base** | 2-4x A100 40GB | 1x A100 40GB (减小batch) |
| **Large** | 8x A100 80GB | 4x A100 80GB (增大梯度累积) |

### 5.4 显存优化技巧

1. **Activation Checkpointing**
   - 以计算换显存
   - 可节省30-40%显存
   - 训练速度降低约20%

2. **DeepSpeed ZeRO**
   - ZeRO-1: 优化器状态分片
   - ZeRO-2: 梯度分片
   - ZeRO-3: 参数分片

3. **正交随机注意力特有优化**
   - K,V正交矩阵预计算并缓存
   - 使用快速正交矩阵生成算法
   - 减少内存访问模式的不规则性

---

## 6. 实验配置汇总表

### 6.1 Small模型完整配置

```yaml
model:
  layers: 6
  hidden_dim: 512
  num_heads: 8
  ffn_dim: 2048
  dropout: 0.1
  max_seq_len: 512
  vocab_size: 32000

optimizer:
  type: AdamW
  lr: 5e-4
  betas: [0.9, 0.98]
  eps: 1e-8
  weight_decay: 0.01
  grad_clip: 1.0

lr_schedule:
  warmup_steps: 4000
  warmup_strategy: linear
  decay_strategy: cosine
  min_lr_ratio: 0.1
  total_steps: 100000

training:
  batch_size: 64
  gradient_accumulation: 1
  effective_batch_size: 64
  max_steps: 100000
  mixed_precision: fp16
  seq_length: 512
  eval_interval: 5000
  save_interval: 10000

resources:
  gpu: "1x A100 40GB"
  estimated_time: "6.6 hours"
  memory: "1.54 GB"
```

### 6.2 Base模型完整配置

```yaml
model:
  layers: 12
  hidden_dim: 768
  num_heads: 12
  ffn_dim: 3072
  dropout: 0.1
  max_seq_len: 512
  vocab_size: 32000

optimizer:
  type: AdamW
  lr: 3e-4
  betas: [0.9, 0.98]
  eps: 1e-8
  weight_decay: 0.01
  grad_clip: 1.0

lr_schedule:
  warmup_steps: 8000
  warmup_strategy: linear
  decay_strategy: cosine_with_restarts
  min_lr_ratio: 0.1
  total_steps: 300000
  restart_interval: 100000

training:
  batch_size: 32
  gradient_accumulation: 2
  effective_batch_size: 64
  max_steps: 300000
  mixed_precision: fp16
  seq_length: 512
  eval_interval: 10000
  save_interval: 20000

resources:
  gpu: "4x A100 40GB"
  estimated_time: "4.2 hours"
  memory: "2.03 GB per GPU"
```

### 6.3 Large模型完整配置

```yaml
model:
  layers: 24
  hidden_dim: 1024
  num_heads: 16
  ffn_dim: 4096
  dropout: 0.1
  max_seq_len: 512
  vocab_size: 32000

optimizer:
  type: AdamW
  lr: 2e-4
  betas: [0.9, 0.98]
  eps: 1e-8
  weight_decay: 0.01
  grad_clip: 1.0

lr_schedule:
  warmup_steps: 12000
  warmup_strategy: linear
  decay_strategy: polynomial
  min_lr_ratio: 0.05
  total_steps: 500000
  power: 2.0

training:
  batch_size: 16
  gradient_accumulation: 4
  effective_batch_size: 64
  max_steps: 500000
  mixed_precision: bf16
  seq_length: 512
  eval_interval: 10000
  save_interval: 25000

resources:
  gpu: "8x A100 80GB"
  estimated_time: "3.0 hours"
  memory: "4.04 GB per GPU"
```

---

## 7. 训练监控与调试

### 7.1 关键监控指标

| 指标 | 正常范围 | 异常信号 |
|------|----------|----------|
| **Loss** | 持续下降 | 突然上升/NaN |
| **Learning Rate** | 按调度变化 | 超出预期范围 |
| **Gradient Norm** | < 10 | > 100 (梯度爆炸) |
| **Throughput** | 稳定 | 突然下降 |
| **GPU Memory** | 稳定 | OOM错误 |

### 7.2 调试建议

1. **Loss不收敛**
   - 降低学习率
   - 增加warmup步数
   - 检查数据预处理

2. **显存溢出**
   - 减小batch_size
   - 增加梯度累积
   - 启用activation checkpointing

3. **训练速度慢**
   - 检查数据加载瓶颈
   - 增加dataloader workers
   - 使用pin_memory

---

## 8. 附录

### 8.1 正交随机矩阵生成

```python
import torch

def generate_orthogonal_matrix(dim, device="cuda"):
    """Generate Haar random orthogonal matrix"""
    # Use QR decomposition to generate random orthogonal matrix
    A = torch.randn(dim, dim, device=device)
    Q, R = torch.linalg.qr(A)
    # Ensure determinant is 1
    d = torch.diag(R)
    ph = d / torch.abs(d)
    Q = Q * ph
    return Q
```

### 8.2 配置文件模板

```python
# config.py
from dataclasses import dataclass

@dataclass
class ModelConfig:
    layers: int = 12
    hidden_dim: int = 768
    num_heads: int = 12
    ffn_dim: int = 3072
    dropout: float = 0.1
    max_seq_len: int = 512
    vocab_size: int = 32000

@dataclass
class TrainingConfig:
    batch_size: int = 32
    gradient_accumulation: int = 2
    max_steps: int = 300000
    mixed_precision: str = "fp16"
    eval_interval: int = 10000
    save_interval: int = 20000
```

---

*文档版本: 1.0*
*创建日期: 2024*
*适用项目: 正交随机注意力研究*
