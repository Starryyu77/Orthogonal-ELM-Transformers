# 正交随机注意力实验 - 数据集选择方案

## 1. 概述

本方案为正交随机注意力（Orthogonal Random Attention）Transformer新范式的预训练实验设计，提供分阶段、可扩展的数据集选择策略。

### 1.1 实验阶段规划

| 阶段 | 数据集 | 规模 | 目的 | 预计训练时间 |
|------|--------|------|------|-------------|
| 阶段一 | TinyStories | ~2B tokens | 快速验证架构可行性 | 数小时-1天 |
| 阶段二 | OpenWebText | ~25B tokens | 中等规模性能验证 | 3-7天 |
| 阶段三 | SlimPajama/C4 | 100B+ tokens | 大规模性能对比 | 2-4周 |

---

## 2. 数据集推荐与分析

### 2.1 TinyStories - 快速验证阶段

#### 基本信息

| 属性 | 详情 |
|------|------|
| **规模** | 约2-3B tokens |
| **文档数** | 约200万篇短篇故事 |
| **来源** | 由GPT-4生成的简单英语故事 |
| **领域** | 儿童故事、简单叙事 |
| **HuggingFace** | `roneneldan/TinyStories` |

#### 特点分析

**优势：**
- ✅ **训练速度快**：小模型（<10M参数）可在单GPU数小时内完成训练
- ✅ **验证周期短**：适合快速迭代和超参数调优
- ✅ **语言简单**：词汇量小（约10K tokens），易于学习
- ✅ **可解释性强**：故事结构清晰，便于分析注意力模式
- ✅ **计算成本低**：适合验证新架构的基本可行性

**局限性：**
- ⚠️ 领域单一，无法测试通用语言能力
- ⚠️ 数据由AI生成，与真实分布有差异
- ⚠️ 不适合评估复杂推理能力

#### 适用场景

1. **架构可行性验证**：验证正交随机注意力是否能正常收敛
2. **超参数搜索**：快速确定学习率、batch size等关键参数
3. **消融实验**：比较不同组件的效果
4. **注意力模式分析**：研究正交随机注意力的注意力分布特性

#### 推荐配置

```python
# TinyStories 训练配置
tinystories_config = {
    "seq_length": 256,      # 故事较短，256足够
    "batch_size": 512,      # 可较大batch加速
    "learning_rate": 1e-3,
    "warmup_steps": 100,
    "max_steps": 10000,     # 约2B tokens
    "vocab_size": 10000,    # 使用精简词汇表
}
```

---

### 2.2 OpenWebText - 中等规模验证

#### 基本信息

| 属性 | 详情 |
|------|------|
| **规模** | ~40GB文本，约8M文档 |
| **Token数** | 约20-25B tokens (GPT-2 BPE) |
| **来源** | Reddit外链网页文本 |
| **领域** | 多样化网页内容 |
| **HuggingFace** | `openwebtext` |

#### 特点分析

**优势：**
- ✅ **规模适中**：足够训练中等规模模型（100M-1B参数）
- ✅ **领域多样**：包含新闻、博客、百科等多种内容
- ✅ **质量较高**：经过Reddit社区筛选的高质量内容
- ✅ **基准丰富**：可与GPT-2等模型直接对比
- ✅ **社区支持**：预处理工具成熟，易于使用

**局限性：**
- ⚠️ 英文为主，多语言能力有限
- ⚠️ 可能存在一定偏见
- ⚠️ 不包含代码、学术文献等专业内容

#### 适用场景

1. **中等规模模型训练**：100M-1B参数模型的完整预训练
2. **性能基准测试**：与标准Transformer架构对比
3. **迁移学习评估**：测试下游任务表现
4. **注意力机制分析**：研究正交随机注意力在多样化内容上的表现

#### 推荐配置

```python
# OpenWebText 训练配置
openwebtext_config = {
    "seq_length": 1024,     # 标准长度
    "batch_size": 64,       # 根据GPU显存调整
    "learning_rate": 6e-4,
    "warmup_steps": 2000,
    "max_steps": 50000,     # 约25B tokens
    "weight_decay": 0.1,
}
```

---

### 2.3 SlimPajama - 大规模验证（推荐）

#### 基本信息

| 属性 | 详情 |
|------|------|
| **规模** | 627B tokens |
| **原始数据** | 从RedPajama 1.2T tokens去重得到 |
| **去重率** | 约50%字节级去重 |
| **HuggingFace** | `cerebras/SlimPajama-627B` |

#### 数据组成

| 数据源 | 占比 | 说明 |
|--------|------|------|
| CommonCrawl | 52.2% | 网页文本 |
| C4 | 26.7% | 清洗后的Common Crawl |
| GitHub | 5.2% | 代码数据 |
| Books | 4.2% | 书籍文本 |
| ArXiv | 4.6% | 学术论文 |
| Wikipedia | 3.8% | 百科内容 |
| StackExchange | 3.3% | 问答数据 |

#### 特点分析

**优势：**
- ✅ **高质量去重**：全局MinHashLSH去重，减少冗余
- ✅ **多样性丰富**：涵盖网页、代码、学术、书籍等多领域
- ✅ **规模充足**：627B tokens足够训练7B+参数模型
- ✅ **实证验证**：已用于训练多个高性能模型（BTLM-3B等）
- ✅ **开源可用**：完全开放，可复现

**局限性：**
- ⚠️ 数据量大，需要大量存储和计算资源
- ⚠️ 预处理时间较长
- ⚠️ 主要面向英文

#### 适用场景

1. **大规模模型训练**：1B+参数模型的完整预训练
2. **Scaling Law研究**：验证正交随机注意力的扩展性
3. **多领域评估**：测试模型在不同领域的泛化能力
4. **生产级验证**：接近实际部署场景的性能测试

#### 推荐配置

```python
# SlimPajama 训练配置
slimpajama_config = {
    "seq_length": 2048,     # 长序列支持
    "batch_size": 32,       # 大规模训练
    "learning_rate": 1.5e-4,
    "warmup_steps": 5000,
    "max_steps": 200000,    # 100B+ tokens
    "weight_decay": 0.1,
}
```

---

### 2.4 C4 - 备选大规模数据集

#### 基本信息

| 属性 | 详情 |
|------|------|
| **规模** | 约156B-365B tokens |
| **文档数** | 约3.65亿文档 |
| **来源** | Common Crawl清洗版 |
| **HuggingFace** | `c4` |

#### 特点分析

**优势：**
- ✅ **广泛使用**：T5、MosaicBERT等多个模型的训练数据
- ✅ **质量可控**：经过多重清洗和过滤
- ✅ **易于获取**：HuggingFace直接可用
- ✅ **验证集标准**：C4验证集是语言建模的标准基准

**局限性：**
- ⚠️ 相比SlimPajama，多样性较低
- ⚠️ 去重程度不如SlimPajama
- ⚠️ 主要面向网页文本

#### 适用场景

1. **快速大规模实验**：SlimPajama下载/处理时间过长时的替代
2. **C4验证集评估**：需要在标准C4验证集上评估
3. **资源受限场景**：存储/计算资源有限时

---

### 2.5 数据集对比总结

| 数据集 | 规模 | 多样性 | 质量 | 速度 | 适用阶段 |
|--------|------|--------|------|------|----------|
| TinyStories | ⭐⭐ | ⭐ | ⭐⭐⭐ | ⭐⭐⭐ | 快速验证 |
| OpenWebText | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | 中等验证 |
| C4 | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | 大规模验证 |
| SlimPajama | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐ | 大规模验证 |

---

## 3. Tokenizer选择

### 3.1 推荐方案

| 实验阶段 | 推荐Tokenizer | 词表大小 | 理由 |
|----------|--------------|----------|------|
| TinyStories | GPT-2/GPT-Neo | 10K-50K | 简单高效，与基线一致 |
| OpenWebText | GPT-2 | 50,257 | 与GPT-2对比公平 |
| SlimPajama | GPT-NeoX | 50,400+ | 与Pythia等模型一致 |

### 3.2 Tokenizer代码示例

```python
from transformers import AutoTokenizer

# TinyStories - 精简词表
def get_tinystories_tokenizer():
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    # 只保留最频繁的10K tokens
    tokenizer_vocab = dict(tokenizer.vocab)
    sorted_tokens = sorted(tokenizer_vocab.items(), key=lambda x: x[1])
    limited_vocab = {token: idx for idx, (token, _) in enumerate(sorted_tokens[:10000])}
    # 重新映射
    tokenizer.vocab = limited_vocab
    return tokenizer

# OpenWebText - 标准GPT-2
def get_openwebtext_tokenizer():
    return AutoTokenizer.from_pretrained("gpt2")

# SlimPajama - GPT-NeoX
def get_slimpajama_tokenizer():
    return AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")
```

---

## 4. 数据预处理方案

### 4.1 预处理流程

```
原始数据 → 文本清洗 → Tokenization → 序列打包 → 数据加载
```

### 4.2 预处理代码示例

```python
import os
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer
from tqdm import tqdm

def preprocess_dataset(
    dataset_name: str,
    tokenizer_name: str,
    output_dir: str,
    seq_length: int = 1024,
    num_proc: int = 8
):
    """
    预处理预训练数据集
    
    Args:
        dataset_name: HuggingFace数据集名称
        tokenizer_name: Tokenizer名称
        output_dir: 输出目录
        seq_length: 序列长度
        num_proc: 并行进程数
    """
    
    # 加载数据集
    ds = load_dataset(dataset_name, split="train", streaming=True)
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Tokenization函数
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=seq_length,
            return_special_tokens_mask=False,
        )
    
    # 处理数据
    all_tokens = []
    for batch in tqdm(ds, desc="Tokenizing"):
        tokens = tokenize_function(batch)
        all_tokens.extend(tokens["input_ids"])
    
    # 打包成固定长度序列
    num_sequences = len(all_tokens) // seq_length
    packed_tokens = np.array(all_tokens[:num_sequences * seq_length])
    packed_tokens = packed_tokens.reshape(-1, seq_length)
    
    # 保存为内存映射文件
    os.makedirs(output_dir, exist_ok=True)
    
    train_path = os.path.join(output_dir, "train.bin")
    val_path = os.path.join(output_dir, "val.bin")
    
    # 划分训练/验证集 (95%/5%)
    split_idx = int(0.95 * len(packed_tokens))
    train_tokens = packed_tokens[:split_idx]
    val_tokens = packed_tokens[split_idx:]
    
    # 保存
    train_tokens.astype(np.uint16).tofile(train_path)
    val_tokens.astype(np.uint16).tofile(val_path)
    
    print(f"Saved {len(train_tokens)} train sequences to {train_path}")
    print(f"Saved {len(val_tokens)} val sequences to {val_path}")
    
    return train_path, val_path


# 使用示例
if __name__ == "__main__":
    # TinyStories预处理
    preprocess_dataset(
        dataset_name="roneneldan/TinyStories",
        tokenizer_name="gpt2",
        output_dir="./data/tinystories",
        seq_length=256,
    )
```

### 4.3 数据加载器

```python
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader

class PretrainingDataset(Dataset):
    """
    预训练数据集加载器
    """
    def __init__(self, data_path: str, seq_length: int):
        self.data = np.memmap(data_path, dtype=np.uint16, mode='r')
        self.seq_length = seq_length
        self.num_sequences = len(self.data) // seq_length
        
    def __len__(self):
        return self.num_sequences
    
    def __getitem__(self, idx):
        start = idx * self.seq_length
        end = start + self.seq_length + 1  # +1 for target
        
        tokens = torch.from_numpy(self.data[start:end].astype(np.int64))
        x = tokens[:-1]  # 输入
        y = tokens[1:]   # 目标
        
        return {"input_ids": x, "labels": y}


def get_dataloader(
    data_path: str,
    seq_length: int,
    batch_size: int,
    num_workers: int = 4,
    shuffle: bool = True,
):
    """
    创建数据加载器
    """
    dataset = PretrainingDataset(data_path, seq_length)
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True,
    )
```

---

## 5. 实验阶段数据策略

### 5.1 阶段一：快速验证（TinyStories）

**目标**：验证正交随机注意力架构能否正常收敛

| 配置项 | 值 | 说明 |
|--------|-----|------|
| 模型大小 | 1M-33M参数 | 小模型快速迭代 |
| 序列长度 | 256 | 故事较短 |
| Batch Size | 512 | 可较大加速 |
| 训练Tokens | 2B | 完整数据集 |
| 训练时间 | 数小时 | 单GPU |

**验证指标**：
- 训练/验证loss是否正常下降
- 生成故事是否连贯
- 注意力权重分布是否合理

### 5.2 阶段二：中等验证（OpenWebText）

**目标**：与标准Transformer进行公平对比

| 配置项 | 值 | 说明 |
|--------|-----|------|
| 模型大小 | 124M-355M参数 | GPT-2 small/medium |
| 序列长度 | 1024 | 标准长度 |
| Batch Size | 64-128 | 根据显存调整 |
| 训练Tokens | 10B-25B | 部分或完整数据 |
| 训练时间 | 3-7天 | 8x A100 |

**验证指标**：
- 与GPT-2在相同数据上的loss对比
- 下游任务zero-shot性能
- 推理速度对比

### 5.3 阶段三：大规模验证（SlimPajama）

**目标**：验证Scaling Law和实际部署价值

| 配置项 | 值 | 说明 |
|--------|-----|------|
| 模型大小 | 1B-7B参数 | 大规模模型 |
| 序列长度 | 2048 | 长上下文 |
| Batch Size | 32-64 | 大规模训练 |
| 训练Tokens | 100B-300B | 大规模数据 |
| 训练时间 | 2-4周 | 大规模集群 |

**验证指标**：
- 与Pythia等模型的Scaling Law对比
- 多领域下游任务性能
- 长上下文建模能力
- 训练/推理效率

---

## 6. 采样策略

### 6.1 数据采样方法

```python
import torch
import numpy as np

class DataSampler:
    """
    数据采样器，支持多种采样策略
    """
    
    def __init__(
        self,
        dataset_size: int,
        strategy: str = "random",  # random, sequential, weighted
        seed: int = 42,
    ):
        self.dataset_size = dataset_size
        self.strategy = strategy
        self.rng = np.random.RandomState(seed)
        
    def sample(self, batch_size: int, step: int = 0):
        if self.strategy == "random":
            # 随机采样
            indices = self.rng.randint(0, self.dataset_size, batch_size)
        elif self.strategy == "sequential":
            # 顺序采样
            start = (step * batch_size) % self.dataset_size
            end = min(start + batch_size, self.dataset_size)
            indices = np.arange(start, end)
            if len(indices) < batch_size:
                # 循环
                indices = np.concatenate([
                    indices,
                    np.arange(0, batch_size - len(indices))
                ])
        elif self.strategy == "weighted":
            # 加权采样（用于多数据源混合）
            indices = self.rng.choice(
                self.dataset_size,
                size=batch_size,
                p=self.weights,
            )
        return indices


class MultiSourceSampler:
    """
    多数据源混合采样器
    """
    
    def __init__(
        self,
        source_sizes: dict,  # {source_name: size}
        source_weights: dict,  # {source_name: weight}
        seed: int = 42,
    ):
        self.source_sizes = source_sizes
        self.source_weights = source_weights
        self.rng = np.random.RandomState(seed)
        
        # 归一化权重
        total_weight = sum(source_weights.values())
        self.normalized_weights = {
            k: v / total_weight for k, v in source_weights.items()
        }
        
    def sample(self, batch_size: int):
        """
        按权重从各数据源采样
        """
        samples_per_source = {}
        for source, weight in self.normalized_weights.items():
            n_samples = int(batch_size * weight)
            samples_per_source[source] = n_samples
        
        # 处理舍入误差
        remaining = batch_size - sum(samples_per_source.values())
        if remaining > 0:
            # 随机分配给某些source
            for _ in range(remaining):
                source = self.rng.choice(list(self.normalized_weights.keys()))
                samples_per_source[source] += 1
        
        # 从各source采样
        batch = {}
        for source, n in samples_per_source.items():
            indices = self.rng.randint(0, self.source_sizes[source], n)
            batch[source] = indices
            
        return batch
```

### 6.2 学习率调度与数据配合

```python
def get_lr_schedule(
    optimizer,
    warmup_steps: int,
    max_steps: int,
    min_lr_ratio: float = 0.1,
):
    """
    余弦退火学习率调度
    """
    def lr_lambda(current_step):
        if current_step < warmup_steps:
            # 线性warmup
            return float(current_step) / float(max(1, warmup_steps))
        # 余弦退火
        progress = float(current_step - warmup_steps) / float(max(1, max_steps - warmup_steps))
        return max(min_lr_ratio, 0.5 * (1.0 + np.cos(np.pi * progress)))
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
```

---

## 7. 数据质量控制

### 7.1 质量检查清单

- [ ] **重复数据检查**：使用MinHash检测近似重复
- [ ] **长度过滤**：过滤过短/过长的文档
- [ ] **语言检测**：确保主要语言正确
- [ ] **污染检测**：确保训练/验证集无重叠
- [ ] **特殊字符处理**：清洗控制字符和乱码

### 7.2 去重代码示例

```python
from datasketch import MinHash, MinHashLSH

def deduplicate_documents(
    documents: list,
    threshold: float = 0.8,
    num_perm: int = 128,
):
    """
    使用MinHashLSH进行文档去重
    
    Args:
        documents: 文档列表
        threshold: Jaccard相似度阈值
        num_perm: MinHash置换数
    
    Returns:
        去重后的文档列表
    """
    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)
    unique_docs = []
    
    for idx, doc in enumerate(documents):
        # 创建shingles
        shingles = set()
        words = doc.lower().split()
        for i in range(len(words) - 2):
            shingles.add(" ".join(words[i:i+3]))
        
        # 创建MinHash
        m = MinHash(num_perm=num_perm)
        for s in shingles:
            m.update(s.encode('utf8'))
        
        # 检查是否重复
        result = lsh.query(m)
        if not result:  # 无重复
            lsh.insert(idx, m)
            unique_docs.append(doc)
    
    return unique_docs
```

---

## 8. 实验复现建议

### 8.1 环境配置

```bash
# 创建conda环境
conda create -n ortho-attn python=3.10
conda activate ortho-attn

# 安装依赖
pip install torch transformers datasets accelerate
pip install numpy tqdm wandb
pip install datasketch  # 用于去重
```

### 8.2 快速开始脚本

```bash
#!/bin/bash
# run_experiments.sh

# 阶段一：TinyStories快速验证
echo "Stage 1: TinyStories Quick Validation"
python train.py \
    --dataset tinystories \
    --model_size 33M \
    --seq_length 256 \
    --batch_size 512 \
    --max_steps 10000 \
    --output_dir ./outputs/tinystories

# 阶段二：OpenWebText中等验证
echo "Stage 2: OpenWebText Medium Validation"
python train.py \
    --dataset openwebtext \
    --model_size 124M \
    --seq_length 1024 \
    --batch_size 64 \
    --max_steps 50000 \
    --output_dir ./outputs/openwebtext

# 阶段三：SlimPajama大规模验证
echo "Stage 3: SlimPajama Large Scale Validation"
python train.py \
    --dataset slimpajama \
    --model_size 1B \
    --seq_length 2048 \
    --batch_size 32 \
    --max_steps 200000 \
    --output_dir ./outputs/slimpajama
```

---

## 9. 总结与建议

### 9.1 数据集选择决策树

```
                    开始
                     │
         ┌───────────┴───────────┐
         │                       │
    快速验证?              大规模训练?
         │                       │
         ▼                       ▼
   TinyStories           多样性需求高?
   (数小时)                    │
                          ┌────┴────┐
                          │         │
                         高        一般
                          │         │
                          ▼         ▼
                    SlimPajama    C4
                    (推荐)       (备选)
```

### 9.2 关键建议

1. **分阶段验证**：从TinyStories开始，逐步扩大规模
2. **公平对比**：与标准Transformer使用相同数据和tokenizer
3. **关注效率**：记录训练速度和内存占用
4. **多维度评估**：不仅看loss，还要评估下游任务
5. **可复现性**：固定随机种子，记录所有超参数

### 9.3 预期结果

| 实验阶段 | 成功标准 |
|----------|----------|
| TinyStories | 验证loss < 2.0，生成故事连贯 |
| OpenWebText | 与GPT-2相当或更好的loss |
| SlimPajama | 验证Scaling Law，下游任务性能达标 |

---

## 参考资源

1. **TinyStories**: https://huggingface.co/datasets/roneneldan/TinyStories
2. **OpenWebText**: https://huggingface.co/datasets/openwebtext
3. **SlimPajama**: https://huggingface.co/datasets/cerebras/SlimPajama-627B
4. **C4**: https://huggingface.co/datasets/c4
5. **nanoGPT**: https://github.com/karpathy/nanoGPT (参考实现)
6. **lit-gpt**: https://github.com/Lightning-AI/lit-gpt (高效训练框架)

---

*文档版本: 1.0*
*创建日期: 2025年*
*适用项目: 正交随机注意力实验*
