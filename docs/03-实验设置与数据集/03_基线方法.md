# 正交随机注意力实验 - 基线对比方案

## 1. 研究背景与目标

本实验旨在对比"正交随机注意力"（Orthogonal Random Attention）与现有Transformer变体的性能差异，验证正交初始化在注意力机制中的有效性。

---

## 2. 基线方法选择

### 2.1 基线方法总览表

| 基线方法 | 类型 | 核心特点 | 选择理由 |
|---------|------|---------|---------|
| **Vanilla Transformer** | 标准基线 | 标准QK^T点积注意力 | 最广泛使用的基准 |
| **Random Gaussian QK** | 消融基线 | 高斯随机初始化QK后冻结 | 验证正交vs高斯的效果 |
| **Synthesizer (Random)** | 方法对比 | 随机注意力矩阵 | 验证token-token交互必要性 |
| **Synthesizer (Factorized)** | 方法对比 | 低秩分解随机矩阵 | 对比低秩近似效果 |
| **Partial Frozen Strategy** | 渐进式 | 部分层冻结训练 | 验证渐进式训练策略 |
| **Fixed Random Attention** | 极端基线 | 完全固定的随机注意力 | 验证可训练性影响 |

### 2.2 各基线方法详细说明

#### 2.2.1 Vanilla Transformer（标准Transformer）

**数学形式：**
```
Attention(Q,K,V) = softmax(QK^T / √d_k)V
其中 Q = XW_Q, K = XW_K, V = XW_V
```

**选择理由：**
- 所有Transformer变体的标准对比基准
- 证明dot-product attention的普适性
- 提供性能上界参考

**配置要点：**
- 使用标准Xavier/Glorot初始化
- 完整的Q、K、V投影矩阵可训练
- 标准多头注意力机制

---

#### 2.2.2 Random Gaussian QK（高斯随机QK基线）

**数学形式：**
```
Q = XW_Q^fixed, K = XW_K^fixed
W_Q, W_K ~ N(0, σ²) 初始化后冻结
Attention = softmax(QK^T / √d_k)V
```

**选择理由：**
- 直接与正交初始化对比
- 验证正交性是否优于高斯随机
- 控制变量：仅改变初始化分布

**配置要点：**
- Q、K投影矩阵使用高斯分布初始化
- 初始化后完全冻结（不训练）
- V矩阵保持可训练

---

#### 2.2.3 Synthesizer (Random)

**数学形式：**
```
Y = softmax(R)G(X)
其中 R ∈ R^(N×N) 为随机初始化矩阵
G(X) 为Value投影
```

**选择理由：**
- Google Research提出的随机注意力方法
- 验证"token-token交互是否必要"
- 直接对比随机注意力矩阵的效果

**变体设置：**
- **Random Trainable**: R可训练
- **Random Fixed**: R完全固定

---

#### 2.2.4 Synthesizer (Factorized)

**数学形式：**
```
Y = softmax(R_1 R_2^T)G(X)
其中 R_1, R_2 ∈ R^(N×k), k << N
```

**选择理由：**
- 低秩近似减少参数量
- 对比不同复杂度下的性能
- 验证正交初始化在低秩场景的效果

---

#### 2.2.5 Partial Frozen Strategy（部分冻结策略）

**训练阶段：**

| 阶段 | 冻结部分 | 可训练部分 | 训练步数 |
|------|---------|-----------|---------|
| 阶段1 | Q,K矩阵 | V, FFN, LayerNorm | 50%总步数 |
| 阶段2 | 前50%层的Q,K | 后50%层全部 + 其他 | 30%总步数 |
| 阶段3 | 无 | 全部参数 | 20%总步数 |

**选择理由：**
- 渐进式解冻策略
- 验证分阶段训练的效果
- 模拟预训练-微调范式

---

#### 2.2.6 Fixed Random Attention（固定随机注意力）

**数学形式：**
```
Attention = softmax(R_fixed)V
R_fixed ~ 正交/高斯随机，完全不可训练
```

**选择理由：**
- 极端情况验证
- 测试注意力矩阵的可训练性影响
- 提供性能下界参考

---

## 3. 消融实验设计

### 3.1 消融实验矩阵

| 实验编号 | 实验名称 | 变量 | 控制条件 |
|---------|---------|------|---------|
| A1 | 正交性 vs 高斯随机 | 初始化分布 | 冻结QK，可训练V |
| A2 | 冻结 vs 可训练QK | QK可训练性 | 正交初始化 |
| A3 | 不同头数影响 | 注意力头数 H | 固定总维度 d_model |
| A4 | 不同维度影响 | 维度 d_k | 固定头数 H |
| A5 | 不同序列长度 | 序列长度 N | 固定模型配置 |
| A6 | 不同层数影响 | 层数 L | 固定每层配置 |
| A7 | 正交化方法对比 | QR/SVD/Householder | 冻结QK |

### 3.2 消融实验详细配置

#### 实验A1: 正交性 vs 高斯随机

**目的：** 验证正交初始化是否优于标准高斯初始化

**配置：**
```yaml
方法1 - 正交随机:
  Q_init: orthogonal
  K_init: orthogonal
  Q_trainable: False
  K_trainable: False

方法2 - 高斯随机:
  Q_init: normal(mean=0, std=0.02)
  K_init: normal(mean=0, std=0.02)
  Q_trainable: False
  K_trainable: False

方法3 - 均匀随机:
  Q_init: uniform(-0.02, 0.02)
  K_init: uniform(-0.02, 0.02)
  Q_trainable: False
  K_trainable: False
```

**评估指标：** 最终loss差距、收敛速度、稳定性

---

#### 实验A2: 冻结 vs 可训练QK

**目的：** 验证QK矩阵可训练性的影响

**配置：**
```yaml
配置1 - 完全冻结:
  Q_trainable: False
  K_trainable: False
  
配置2 - 仅Q可训练:
  Q_trainable: True
  K_trainable: False
  
配置3 - 仅K可训练:
  Q_trainable: False
  K_trainable: True
  
配置4 - QK都可训练:
  Q_trainable: True
  K_trainable: True
  
配置5 - 标准Transformer:
  QK都可训练 + 标准初始化
```

---

#### 实验A3: 不同头数影响

**目的：** 验证注意力头数对正交随机注意力的影响

**配置（固定d_model=512）：**

| 配置 | 头数 H | 每头维度 d_k | 总参数量 |
|-----|-------|------------|---------|
| H2 | 2 | 256 | 基准 |
| H4 | 4 | 128 | 相同 |
| H8 | 8 | 64 | 相同 |
| H16 | 16 | 32 | 相同 |

**关键观察：** 正交性在不同维度下的保持效果

---

#### 实验A4: 不同维度影响

**目的：** 验证模型维度对正交随机注意力的影响

**配置（固定H=8）：**

| 配置 | d_model | d_k | 层数 | 参数量 |
|-----|---------|-----|-----|-------|
| Tiny | 256 | 32 | 4 | 小 |
| Small | 512 | 64 | 6 | 中 |
| Base | 768 | 96 | 8 | 大 |
| Large | 1024 | 128 | 12 | 更大 |

---

#### 实验A5: 不同序列长度

**目的：** 验证正交随机注意力对不同序列长度的适应性

**配置：**

| 序列长度 | 应用场景 | 正交矩阵大小 |
|---------|---------|-------------|
| 128 | 短文本分类 | 128×128 |
| 512 | 标准NLP任务 | 512×512 |
| 1024 | 长文档 | 1024×1024 |
| 2048 | 长序列建模 | 2048×2048 |

---

#### 实验A6: 不同层数影响

**目的：** 验证正交随机注意力在不同深度网络中的表现

**配置（固定每层d_model=512）：**

| 配置 | 层数 | 总参数量 | 应用场景 |
|-----|-----|---------|---------|
| Shallow | 4 | 小 | 简单任务 |
| Medium | 8 | 中 | 标准任务 |
| Deep | 12 | 大 | 复杂任务 |
| Very Deep | 24 | 很大 | 高难度任务 |

---

#### 实验A7: 正交化方法对比

**目的：** 对比不同正交化实现方式的性能

**配置：**
```yaml
方法1 - QR分解:
  orthogonalization: QR
  特点: 数值稳定，常用

方法2 - SVD分解:
  orthogonalization: SVD
  特点: 精确但计算量大

方法3 - Householder变换:
  orthogonalization: Householder
  特点: 参数化正交矩阵

方法4 - Cayley变换:
  orthogonalization: Cayley
  特点: 保持正交性梯度
```

---

## 4. 公平对比原则

### 4.1 模型规模控制

**原则：** 所有对比方法必须具有相同的模型参数量

**控制方法：**

| 组件 | 控制策略 |
|-----|---------|
| 总层数 | 所有方法使用相同的L |
| 隐藏维度 | 所有方法使用相同的d_model |
| FFN维度 | 所有方法使用相同的d_ff |
| 注意力头数 | 所有方法使用相同的H |
| 词汇表大小 | 所有方法使用相同的Vocab |

**参数量计算公式：**
```
总参数量 = L × [4 × d_model² (QKVO) + 2 × d_model × d_ff (FFN) + 2 × d_model (LayerNorm)]
```

### 4.2 训练数据控制

**原则：** 所有方法使用完全相同的训练数据

**控制清单：**
- [ ] 相同的数据集（如WikiText-103, C4等）
- [ ] 相同的数据预处理流程
- [ ] 相同的tokenizer
- [ ] 相同的数据划分（train/val/test）
- [ ] 相同的batch size
- [ ] 相同的序列长度

### 4.3 训练步数控制

**原则：** 所有方法训练相同的步数或使用相同的早停策略

**配置：**
```yaml
训练配置:
  max_steps: 100000
  warmup_steps: 4000
  eval_interval: 1000
  save_interval: 5000
  
早停策略:
  patience: 10
  min_delta: 0.001
  monitor: val_loss
```

### 4.4 随机种子控制

**原则：** 使用相同的随机种子确保可复现性

**种子设置：**
```python
seeds = [42, 2024, 12345, 98765, 555666]
# 每个种子独立运行，报告平均值±标准差
```

### 4.5 优化器与学习率

**统一配置：**
```yaml
optimizer:
  type: AdamW
  lr: 1e-4
  betas: [0.9, 0.98]
  eps: 1e-9
  weight_decay: 0.01
  
scheduler:
  type: cosine_with_warmup
  warmup_steps: 4000
  min_lr_ratio: 0.1
```

---

## 5. 评估指标

### 5.1 主要评估指标

| 指标类别 | 具体指标 | 说明 |
|---------|---------|------|
| **性能指标** | Perplexity (PPL) | 语言建模质量 |
| | BLEU Score | 机器翻译质量 |
| | Accuracy | 分类任务准确率 |
| | F1 Score | 综合性能评估 |
| **效率指标** | Training Time | 训练时间 |
| | Inference Speed | 推理速度 |
| | Memory Usage | 显存占用 |
| | FLOPs | 计算量 |
| **稳定性指标** | Loss Variance | 损失方差 |
| | Gradient Norm | 梯度范数 |
| | Convergence Speed | 收敛速度 |

### 5.2 统计显著性检验

**方法：**
- 多次运行（至少5个不同种子）
- 计算均值和标准差
- 使用t-test或Wilcoxon signed-rank test
- 报告p-value

---

## 6. 实验配置表

### 6.1 模型架构配置

| 配置项 | Vanilla | Orthogonal Random | Gaussian Random | Synthesizer | Partial Frozen |
|-------|---------|------------------|-----------------|-------------|----------------|
| d_model | 512 | 512 | 512 | 512 | 512 |
| num_layers | 6 | 6 | 6 | 6 | 6 |
| num_heads | 8 | 8 | 8 | 8 | 8 |
| d_ff | 2048 | 2048 | 2048 | 2048 | 2048 |
| dropout | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 |
| max_seq_len | 512 | 512 | 512 | 512 | 512 |
| vocab_size | 32000 | 32000 | 32000 | 32000 | 32000 |

### 6.2 训练超参数配置

| 超参数 | 值 | 说明 |
|-------|-----|------|
| batch_size | 32 | 每设备batch大小 |
| gradient_accumulation | 4 | 梯度累积步数 |
| effective_batch_size | 128 | 有效batch大小 |
| max_steps | 100000 | 最大训练步数 |
| learning_rate | 1e-4 | 初始学习率 |
| warmup_steps | 4000 | 预热步数 |
| weight_decay | 0.01 | 权重衰减 |
| max_grad_norm | 1.0 | 梯度裁剪 |
| fp16 | True | 混合精度训练 |

### 6.3 各方法特殊配置

#### Orthogonal Random Attention
```yaml
special_config:
  Q_init_method: orthogonal  # 正交初始化
  K_init_method: orthogonal
  Q_trainable: false
  K_trainable: false
  V_trainable: true
  orthogonalization: QR  # QR/SVD/Householder
  freeze_schedule: none  # none/gradual/staged
```

#### Gaussian Random QK
```yaml
special_config:
  Q_init_method: normal
  K_init_method: normal
  Q_init_std: 0.02
  K_init_std: 0.02
  Q_trainable: false
  K_trainable: false
  V_trainable: true
```

#### Synthesizer Random
```yaml
special_config:
  attention_type: random_synthesizer
  R_init_method: normal
  R_trainable: true  # true/false
  R_shape: [max_seq_len, max_seq_len]
  factorized: false
```

#### Synthesizer Factorized
```yaml
special_config:
  attention_type: factorized_synthesizer
  rank: 64  # 低秩维度
  R1_shape: [max_seq_len, rank]
  R2_shape: [max_seq_len, rank]
  R_trainable: true
```

#### Partial Frozen Strategy
```yaml
special_config:
  freeze_schedule:
    stage1:
      steps: 50000
      frozen: [Q, K]
      trainable: [V, FFN, LN]
    stage2:
      steps: 30000
      frozen: [Q_layers_0-3, K_layers_0-3]
      trainable: [all_layers_4-5, V, FFN, LN]
    stage3:
      steps: 20000
      frozen: []
      trainable: [all]
```

---

## 7. 实验执行计划

### 7.1 实验阶段划分

| 阶段 | 内容 | 预计时间 | 优先级 |
|-----|------|---------|-------|
| 阶段1 | 核心对比实验（Vanilla vs Orthogonal vs Gaussian） | 3-5天 | P0 |
| 阶段2 | Synthesizer对比实验 | 2-3天 | P1 |
| 阶段3 | 消融实验A1-A4 | 4-6天 | P0 |
| 阶段4 | 消融实验A5-A7 | 3-4天 | P1 |
| 阶段5 | 不同规模实验 | 5-7天 | P2 |
| 阶段6 | 多任务迁移实验 | 3-5天 | P2 |

### 7.2 资源需求估算

| 实验类型 | GPU需求 | 内存需求 | 预计时间 |
|---------|--------|---------|---------|
| 小规模调试 | 1×A100 40GB | 32GB | 数小时 |
| 标准对比实验 | 4×A100 40GB | 128GB | 3-5天 |
| 大规模实验 | 8×A100 80GB | 256GB | 1-2周 |

---

## 8. 预期结果与分析框架

### 8.1 预期对比结果

| 对比组 | 预期结果 | 分析角度 |
|-------|---------|---------|
| Orthogonal vs Gaussian | Orthogonal略优或相当 | 初始化质量影响 |
| Orthogonal vs Vanilla | Vanilla略优，但差距小 | 可训练性vs效率权衡 |
| Orthogonal vs Synthesizer | Orthogonal更优 | 保留部分结构信息 |
| Frozen vs Trainable | Trainable略优 | 可训练性的价值 |

### 8.2 分析框架

1. **性能分析**：对比最终指标差距
2. **收敛分析**：对比收敛速度和稳定性
3. **效率分析**：对比训练和推理效率
4. **可扩展性分析**：对比不同规模下的表现
5. **任务迁移分析**：对比跨任务泛化能力

---

## 9. 风险与缓解策略

| 风险 | 影响 | 缓解策略 |
|-----|------|---------|
| 正交初始化数值不稳定 | 训练失败 | 使用稳定的QR分解实现 |
| 冻结QK导致欠拟合 | 性能下降 | 增加可训练的V矩阵容量 |
| 长序列内存溢出 | 无法实验 | 使用梯度检查点和序列并行 |
| 训练时间过长 | 进度延迟 | 使用混合精度和分布式训练 |

---

## 10. 附录

### 10.1 正交初始化实现参考

```python
import torch
import torch.nn as nn

def orthogonal_init_(tensor, gain=1.0):
    """正交初始化"""
    if tensor.ndimension() < 2:
        raise ValueError("Only tensors with 2 or more dimensions are supported")
    
    rows = tensor.size(0)
    cols = tensor.numel() // rows
    flattened = tensor.new(rows, cols).normal_(0, 1)
    
    # QR分解获得正交矩阵
    q, r = torch.qr(flattened)
    
    # 确保对角线为正
    d = torch.diag(r, 0)
    ph = d.sign()
    q *= ph.unsqueeze(0)
    
    tensor.view_as(q).copy_(q)
    tensor.mul_(gain)
    return tensor

class OrthogonalRandomAttention(nn.Module):
    """正交随机注意力模块"""
    def __init__(self, d_model, num_heads, freeze_qk=True):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # Q, K使用正交初始化并冻结
        self.W_q = nn.Linear(d_model, d_model, bias=False)
        self.W_k = nn.Linear(d_model, d_model, bias=False)
        
        # 正交初始化
        for w in [self.W_q.weight, self.W_k.weight]:
            orthogonal_init_(w.view(self.num_heads, self.d_k, d_model).transpose(1, 2))
        
        if freeze_qk:
            self.W_q.weight.requires_grad = False
            self.W_k.weight.requires_grad = False
        
        # V保持可训练
        self.W_v = nn.Linear(d_model, d_model, bias=False)
        self.W_o = nn.Linear(d_model, d_model, bias=False)
    
    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.size()
        
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn = torch.softmax(scores, dim=-1)
        context = torch.matmul(attn, V)
        
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        return self.W_o(context)
```

### 10.2 实验记录模板

```yaml
experiment_record:
  experiment_id: "EXP001"
  method: "Orthogonal Random Attention"
  seed: 42
  start_time: "2024-01-15 09:00:00"
  
  model_config:
    d_model: 512
    num_layers: 6
    num_heads: 8
    
  training_config:
    batch_size: 32
    learning_rate: 1e-4
    max_steps: 100000
    
  results:
    final_train_loss: 2.345
    final_val_loss: 2.567
    final_val_ppl: 13.02
    training_time_hours: 48.5
    
  notes: "训练稳定，收敛正常"
```

---

*文档版本: 1.0*
*创建日期: 2024*
*作者: 实验设计Agent*
