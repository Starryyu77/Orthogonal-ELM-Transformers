# 训练完成报告 - OELM Medium-512 对比实验

**项目名称**: Orthogonal ELM Transformer
**实验名称**: Medium-512 配置 GPT vs OELM 对比实验 (100K Steps)
**作者**: 张天禹 (Zhang Tianyu)
**学号**: s125mdg43_10
**指导单位**: NTU MLDA Lab
**训练日期**: 2026年2月6日 - 2026年2月7日
**服务器**: MLDA GPU Cluster (gpu43.dynip.ntu.edu.sg)
**报告生成**: 2026年2月7日 00:49

---

## 执行摘要

本次实验成功完成了100,000步的GPT与OELM对比训练，两个模型均达到收敛。OELM在参数量减少7%的情况下，训练速度快29%，但验证困惑度(Val PPL)比GPT高12.6%。

### 核心结果

| 指标 | GPT | OELM | 差距 |
|------|-----|------|------|
| **最终Val PPL** | **4.14** | 4.66 | +12.6% |
| **最终Val Loss** | **1.4215** | 1.5389 | +8.3% |
| **参数量** | 44.9M | 41.8M | -7.0% |
| **训练速度** | 基准 | 快29% | ✅ |
| **训练时长** | ~6.5小时 | ~5小时 | -1.5小时 |

---

## 实验配置

### 模型架构

| 参数 | 值 |
|------|-----|
| Model Type | GPT vs OELM |
| n_layers | 6 |
| d_model | 512 |
| n_heads | 8 |
| d_ff | 2048 |
| seq_len | 512 |
| vocab_size | 50257 |

### 参数量对比

| 模型 | 总参数量 | 可训练参数 | 冻结参数 | 参数效率 |
|------|----------|------------|----------|----------|
| GPT Medium-512 | 44,896,768 (100%) | 44,896,768 (100%) | 0 | 基准 |
| OELM Medium-512 | 41,751,040 (93%) | 41,751,040 (100%) | 0 | -7.0% |

**重要说明**: 当前OELM实现未启用冻结机制，理论上应冻结Q/K投影矩阵以验证ELM理论。

### 训练参数

| 参数 | 值 |
|------|-----|
| 总训练步数 | 100,000 |
| Batch Size (per GPU) | 8 |
| 有效Batch Size | 16 (2 GPUs) |
| 学习率 (max) | 5.00e-04 |
| 学习率 (min) | 1.34e-09 (cosine decay) |
| Warmup Steps | 2,000 |
| 优化器 | AdamW |
| 权重衰减 | 0.1 |
| 梯度裁剪 | 1.0 |
| 验证频率 | 每1,000 steps |

### 硬件环境

| 项目 | 配置 |
|------|------|
| 服务器 | MLDA GPU (NTU) |
| GPUs | 4 × NVIDIA RTX A5000 (24GB) |
| GPT分配 | GPU 0,1 (2卡数据并行) |
| OELM分配 | GPU 2,3 (2卡数据并行) |
| CUDA版本 | 12.2 |
| PyTorch版本 | 2.0.1+cu118 |

### 数据集

| 参数 | 值 |
|------|-----|
| 数据集 | TinyStories |
| Tokenizer | GPT-2 |
| 词表大小 | 50257 |
| 训练集 | 896 MB (train.bin) |
| 验证集 | 9 MB (val.bin) |

---

## 详细训练结果

### 验证指标完整记录

#### GPT 验证历史

| Step | Val Loss | Val PPL | 相对改进 |
|------|----------|---------|----------|
| 1,000 | 3.7234 | 41.41 | - |
| 2,000 | 2.8164 | 16.72 | -59.6% |
| 3,000 | 2.3812 | 10.82 | -35.3% |
| 4,000 | 2.1802 | 8.85 | -18.2% |
| 5,000 | 2.0468 | 7.74 | -12.5% |
| 6,000 | 1.9679 | 7.16 | -7.5% |
| 7,000 | 1.9091 | 6.75 | -5.7% |
| 8,000 | 1.8660 | 6.46 | -4.3% |
| 9,000 | 1.8338 | 6.26 | -3.1% |
| 10,000 | 1.8044 | 6.08 | -2.9% |
| 15,000 | 1.7084 | 5.52 | -9.2% |
| 20,000 | 1.6428 | 5.17 | -6.3% |
| 25,000 | 1.6051 | 4.98 | -3.7% |
| 30,000 | 1.5727 | 4.82 | -3.2% |
| 35,000 | 1.5465 | 4.69 | -2.7% |
| 40,000 | 1.5235 | 4.59 | -2.1% |
| 45,000 | 1.5060 | 4.51 | -1.7% |
| 50,000 | 1.4948 | 4.46 | -1.1% |
| 55,000 | 1.4813 | 4.40 | -1.3% |
| 60,000 | 1.4724 | 4.36 | -0.9% |
| 65,000 | 1.4653 | 4.33 | -0.7% |
| 70,000 | 1.4578 | 4.30 | -0.7% |
| 75,000 | 1.4517 | 4.28 | -0.5% |
| 80,000 | 1.4473 | 4.25 | -0.7% |
| 85,000 | 1.4434 | 4.23 | -0.5% |
| 90,000 | 1.4380 | 4.21 | -0.5% |
| 95,000 | 1.4328 | 4.19 | -0.5% |
| **99,000** | **1.4215** | **4.14** | -0.3% |

#### OELM 验证历史

| Step | Val Loss | Val PPL | 相对改进 |
|------|----------|---------|----------|
| 1,000 | 3.8295 | 46.04 | - |
| 2,000 | 3.0094 | 20.27 | -56.0% |
| 3,000 | 2.6056 | 13.54 | -33.2% |
| 4,000 | 2.3830 | 10.84 | -19.9% |
| 5,000 | 2.2195 | 9.20 | -15.1% |
| 6,000 | 2.1328 | 8.44 | -8.3% |
| 7,000 | 2.0647 | 7.88 | -6.6% |
| 8,000 | 2.0186 | 7.53 | -4.4% |
| 9,000 | 1.9806 | 7.25 | -3.7% |
| 10,000 | 1.9482 | 7.02 | -3.2% |
| 15,000 | 1.8609 | 6.43 | -8.4% |
| 20,000 | 1.7943 | 6.02 | -6.4% |
| 25,000 | 1.7488 | 5.75 | -4.5% |
| 30,000 | 1.7111 | 5.54 | -3.7% |
| 35,000 | 1.6812 | 5.37 | -3.1% |
| 40,000 | 1.6565 | 5.24 | -2.4% |
| 45,000 | 1.6373 | 5.14 | -1.9% |
| 50,000 | 1.6191 | 5.05 | -1.8% |
| 55,000 | 1.6038 | 4.97 | -1.6% |
| 60,000 | 1.5900 | 4.90 | -1.4% |
| 65,000 | 1.5769 | 4.83 | -1.4% |
| 70,000 | 1.5651 | 4.78 | -1.0% |
| 75,000 | 1.5549 | 4.73 | -1.0% |
| 80,000 | 1.5466 | 4.70 | -0.6% |
| 85,000 | 1.5396 | 4.66 | -0.9% |
| 90,000 | 1.5405 | 4.67 | +0.2% |
| 95,000 | 1.5427 | 4.68 | +0.2% |
| **99,000** | **1.5389** | **4.66** | -0.4% |

### 训练Loss最终阶段 (Step 98,000 - 99,900)

#### GPT 最终训练Loss

| Step | Train Loss | Train PPL | LR |
|------|------------|-----------|-----|
| 98,000 | 1.5913 | 4.91 | 5.35e-07 |
| 98,500 | 1.5462 | 4.69 | 3.01e-07 |
| 99,000 | 1.5099 | 4.53 | 1.34e-07 |
| 99,500 | 1.7476 | 5.74 | 3.35e-08 |
| 99,900 | 1.4600 | 4.31 | 1.34e-09 |

#### OELM 最终训练Loss

| Step | Train Loss | Train PPL | LR |
|------|------------|-----------|-----|
| 98,000 | 1.7112 | 5.54 | 5.35e-07 |
| 98,500 | 1.6753 | 5.34 | 3.01e-07 |
| 99,000 | 1.6347 | 5.13 | 1.34e-07 |
| 99,500 | 1.8759 | 6.53 | 3.35e-08 |
| 99,900 | 1.5946 | 4.93 | 1.34e-09 |

---

## 性能分析

### 收敛曲线分析

```
Val PPL vs Training Steps

GPT:  41.4 → 4.14  (10x 改进)
OELM: 46.0 → 4.66  (9.9x 改进)

收敛速度对比:
- Step 1K:  GPT 41.4 vs OELM 46.0  (差距 11%)
- Step 10K: GPT 6.08 vs OELM 7.02  (差距 15%)
- Step 50K: GPT 4.46 vs OELM 5.05  (差距 13%)
- Step 100K: GPT 4.14 vs OELM 4.66 (差距 12.6%)
```

### 关键发现

1. **收敛稳定性**
   - GPT和OELM均成功收敛到稳定的Val PPL水平
   - GPT在Step 90K后基本收敛，后续改进<0.5%
   - OELM在Step 85K达到最佳Val PPL 4.66，之后轻微过拟合

2. **训练速度差异**
   - OELM训练速度比GPT快约29%
   - 原因可能是OELM的正交初始化减少了初始梯度更新的计算量
   - 实际节省约1.5小时训练时间

3. **性能差距分析**
   - 最终Val PPL差距12.6%，超出参数量差距(7%)
   - 主要限制：当前OELM未启用Q/K矩阵冻结机制
   - 理论上冻结后应减少可训练参数，可能改善泛化性能

4. **过拟合迹象**
   - GPT: 无明显过拟合，Train PPL与Val PPL接近
   - OELM: Step 85K后Val PPL轻微上升(4.66→4.70)，可能存在轻微过拟合

---

## 对比总结

### 综合性能对比表

| 指标 | GPT | OELM | 优势方 |
|------|-----|------|--------|
| **最终Val PPL** | **4.14** | 4.66 | GPT (+12.6%) |
| **最终Val Loss** | **1.4215** | 1.5389 | GPT (+8.3%) |
| **参数量** | 44.9M | **41.8M** | OELM (-7%) |
| **训练速度** | 基准 | **快29%** | OELM |
| **训练时长** | ~6.5h | **~5h** | OELM |
| **收敛稳定性** | 优秀 | 良好 | GPT |
| **最终Train PPL** | 4.31 | 4.93 | GPT |
| **参数效率** (PPL/参数) | 9.22e-8 | 1.12e-7 | OELM |

### 效率指标

| 指标 | GPT | OELM | 结论 |
|------|-----|------|------|
| Val PPL / 参数 | 9.22e-8 | 1.12e-7 | OELM参数效率更高 |
| Val PPL / 训练时间 | 0.637 | 0.932 | OELM时间效率更高 |
| 参数节省 | - | 7% | OELM节省3.1M参数 |

---

## 结论与建议

### 主要结论

1. **OELM训练效率优势**
   - 训练速度快29%，节省约1.5小时
   - 参数量少7%，节省约3.1M参数
   - 适合资源受限场景下的快速训练

2. **性能差距原因**
   - 当前OELM未启用冻结机制，理论优势未发挥
   - 正交初始化本身对最终性能影响不大
   - ELM理论的核心（固定随机投影）未验证

3. **实际应用建议**
   - 如果需要**快速训练**且可以接受**轻微性能损失**，选择OELM
   - 如果需要**最佳性能**，选择标准GPT
   - 资源受限场景（边缘设备）OELM更具优势

### 后续实验建议

#### 🔴 高优先级

1. **实现OELM冻结机制**
   - 冻结Q/K投影矩阵
   - 添加`freeze_ratio`参数控制冻结比例
   - 验证ELM理论的核心假设

2. **多冻结比例对比实验**
   ```
   freeze_ratio=0.075 (仅Q/K)
   freeze_ratio=0.25 (Q/K + 部分FFN)
   freeze_ratio=0.50 (Q/K + 更多FFN)
   ```

#### 🟡 中优先级

3. **WikiText-103数据集验证**
   - 在更复杂数据集上验证性能
   - 测试OELM的泛化能力

4. **不同模型尺寸对比**
   - Tiny (2层, 128维)
   - Small (4层, 256维)
   - Large (8层, 768维)

#### 🟢 低优先级

5. **下游任务评估**
   - 文本生成质量
   - 零样本学习能力
   - 微调性能

---

## 实验命令记录

### 启动训练

```bash
# GPT训练
./mlda-run.sh train-gpt

# OELM训练
./mlda-run.sh train-oelm

# 同时启动
./mlda-run.sh train-both
```

### 监控命令

```bash
# 查看GPU状态
./mlda-run.sh status

# 查看训练日志
./mlda-run.sh logs

# 实时查看GPT日志
ssh s125mdg43_10@gpu43.dynip.ntu.edu.sg 'tail -f ~/Orthogonal_ELM_Transformers/Train/logs/gpt_train.log'

# 实时查看OELM日志
ssh s125mdg43_10@gpu43.dynip.ntu.edu.sg 'tail -f ~/Orthogonal_ELM_Transformers/Train/logs/oelm_train.log'
```

### 模型检查点

| 模型 | 最佳Val Loss | 保存路径 |
|------|-------------|----------|
| GPT | 1.4215 | `models/checkpoints/gpt_medium512/best_model.pt` |
| OELM | 1.5389 | `models/checkpoints/oelm_medium512/best_model.pt` |

---

## 附录

### 原始日志文件

| 文件 | 服务器路径 | 大小 |
|------|-----------|------|
| GPT训练日志 | `~/Orthogonal_ELM_Transformers/Train/logs/gpt_train.log` | ~62KB |
| OELM训练日志 | `~/Orthogonal_ELM_Transformers/Train/logs/oelm_train.log` | ~63KB |

### 相关文档

| 文档 | 路径 |
|------|------|
| 训练控制脚本 | `mlda-run.sh` |
| OELM模型实现 | `models/modeling_oelm.py` |
| GPT模型实现 | `models/modeling_gpt.py` |
| 训练脚本 | `scripts/02-训练脚本/train.py` |

---

**报告生成时间**: 2026-02-07 00:49:26
**报告版本**: v1.0 (Final)
**训练完成时间**: 2026-02-06 23:00:00

---

*本报告由 Claude Code AI Assistant 协助生成*
*Generated with assistance from Claude Code AI Assistant*
