# 训练日志 - OELM Medium-512 对比实验

**项目名称**: Orthogonal ELM Transformer
**实验名称**: Medium-512 配置 GPT vs OELM 对比实验
**作者**: 张天禹 (Zhang Tianyu)
**学号**: s125mdg43_10
**指导单位**: NTU MLDA Lab
**实验日期**: 2026年2月6日
**服务器**: MLDA GPU Cluster (gpu43.dynip.ntu.edu.sg)
**报告生成**: Claude Code AI Assistant

---

## 实验目标

对比标准GPT模型与正交极限学习机Transformer (OELM) 在Medium-512配置下的性能差异，验证OELM在减少参数量的同时保持语言建模能力的效果。

---

## 实验配置

### 模型架构

| 参数 | 值 |
|------|-----|
| Model Type | GPT vs OELM |
| n_layers | 6 |
| d_model | 512 |
| n_heads | 8 |
| d_ff | 2048 |
| seq_len | 512 |
| vocab_size | 50257 |

### 参数量对比

| 模型 | 总参数量 | 可训练参数 | 冻结参数 |
|------|----------|------------|----------|
| GPT Medium-512 | 44.9M (100%) | 44.9M (100%) | 0 |
| OELM Medium-512 | 41.8M (93%) | 41.8M (100%) | 0 |

**注**: 当前OELM实现未启用冻结机制，理论上应冻结Q/K投影矩阵。

### 训练参数

| 参数 | 值 |
|------|-----|
| 总训练步数 | 100,000 |
| Batch Size (per GPU) | 8 |
| 有效Batch Size | 16 (2 GPUs) |
| 学习率 (max) | 3e-4 |
| 学习率 (min) | 3e-5 |
| Warmup Steps | 2,000 |
| 优化器 | AdamW |
| 权重衰减 | 0.1 |
| 梯度裁剪 | 1.0 |

### 硬件环境

| 项目 | 配置 |
|------|------|
| 服务器 | MLDA GPU (NTU) |
| GPUs | 4 × NVIDIA RTX A5000 (24GB) |
| GPT分配 | GPU 0,1 (2卡数据并行) |
| OELM分配 | GPU 2,3 (2卡数据并行) |
| CUDA版本 | 12.2 |
| PyTorch版本 | 2.0.1+cu118 |

### 数据集

| 参数 | 值 |
|------|-----|
| 数据集 | TinyStories |
| Tokenizer | GPT-2 |
| 词表大小 | 50257 |
| 训练集 | 896 MB (train.bin) |
| 验证集 | 9 MB (val.bin) |

---

## 训练时间线

### 启动阶段 (15:30-16:30)

**15:30** - 环境配置
- ✅ 发现MLDA GPU上已有venv环境: `~/projects/oelm/venv`
- ✅ Python 3.8.10, PyTorch 2.0.1+cu118
- ✅ 复制现有数据: `~/projects/oelm/data/tinystories/`

**15:35** - 首次启动尝试，遇到多个问题
- ❌ CUDA OOM (Batch Size 32过大)
- ❌ `--val_data_path` 参数不被支持
- ❌ `--freeze_ratio` 参数不被支持
- ❌ WandB认证失败
- ❌ 模块路径问题 (PYTHONPATH未设置)

**16:15** - 参数调整与修复
- ✅ Batch Size: 32 → 8
- ✅ 移除 `--val_data_path`
- ✅ 移除 `--freeze_ratio`
- ✅ 禁用WandB
- ✅ 添加PYTHONPATH环境变量

**16:31** - 训练成功启动
- ✅ GPT训练启动 (tmux: gpt_train)
- ✅ OELM训练启动 (tmux: oelm_train)
- ✅ 4块GPU全部100%运行

---

## 训练过程记录

### 初始阶段 (Step 0-1000)

| 时间 | Step | GPT Loss | GPT PPL | OELM Loss | OELM PPL | 备注 |
|------|------|----------|---------|-----------|----------|------|
| 16:31 | 0 | 10.93 | 22026 | 10.91 | 22026 | 初始Loss相近 |
| 16:33 | 100 | 9.31 | 11102 | 9.41 | 12175 | GPT收敛略快 |
| 16:35 | 300 | 6.05 | 423.71 | 6.11 | 451.37 | 差距稳定 |
| 16:38 | 1000 | 3.75 | 42.55 | 3.85 | 46.90 | 首次验证 |
| - | - | **3.72** | **41.41** | **3.83** | **46.04** | Val PPL差距11% |

**观察**: OELM在初始阶段表现略逊于GPT，但两者都成功收敛。

### 中期阶段 (Step 1000-3800)

| 时间 | Step | GPT Loss | GPT PPL | OELM Loss | OELM PPL | 备注 |
|------|------|----------|---------|-----------|----------|------|
| 16:40 | 2000 | 3.05 | 21.09 | - | - | GPT继续下降 |
| 16:43 | 3000 | 2.54 | 12.68 | 2.80 | 16.37 | Val Checkpoint |
| - | - | **2.38** | **10.82** | **2.61** | **13.54** | 差距扩大至25% |
| 16:45 | 3200 | 2.53 | 12.60 | - | - | GPT训练Loss |
| 16:45 | - | - | - | 2.50 | 12.13 | OELM训练Loss (Step 3800) |

**关键发现**:
- Step 3000验证: GPT Val PPL=10.82, OELM Val PPL=13.54
- OELM验证PPL比GPT高约25%
- OELM参数量少7%，但性能下降超出预期
- OELM训练速度略快 (Step 3800 vs 3200)

---

## 问题与解决

### 已解决的问题

| 问题 | 原因 | 解决方案 |
|------|------|----------|
| CUDA OOM | Batch Size 32过大 | 调整为Batch Size 8 |
| 模块找不到 | PYTHONPATH未设置 | 添加环境变量 |
| WandB认证失败 | 未登录 | 禁用WandB |
| 参数不支持 | 脚本版本不匹配 | 移除不支持参数 |
| 数据缺失 | 未准备数据 | 复制现有数据 |

### 当前限制

| 限制 | 影响 | 优先级 |
|------|------|--------|
| OELM未启用冻结机制 | 失去架构特性，性能下降 | 🔴 高 |
| 仅在TinyStories测试 | 数据集过于简单 | 🟡 中 |
| 无冻结比例对比实验 | 无法验证ELM理论 | 🔴 高 |

---

## 性能分析

### 当前性能对比 (截至Step 3800)

| 指标 | GPT | OELM | 差距 |
|------|-----|------|------|
| 总参数量 | 44.9M | 41.8M | -7.0% |
| 可训练参数 | 44.9M (100%) | 41.8M (100%) | -7.0% |
| 冻结参数 | 0 | 0 | - |
| **Val Loss** | **2.38** | 2.61 | +9.7% |
| **Val PPL** | **10.82** | 13.54 | **+25.1%** |
| 训练速度 | Step 3200 | Step 3800 | OELM +18% |

### 关键观察

1. **性能差距**: OELM验证PPL比GPT高25%，超出预期
2. **原因分析**:
   - 当前OELM实现未冻结Q/K矩阵
   - 所有参数仍可训练，等同于标准Transformer
   - 正交初始化仅影响初始状态
3. **结论**: **必须实现冻结机制**才能验证ELM理论

---

## 后续实验计划

### 🔴 高优先级 (立即执行)

1. **实现OELM冻结机制**
   - 在`modeling_oelm.py`中实现Q/K矩阵冻结
   - 添加`freeze_ratio`参数
   - 预期: 减少可训练参数，提升训练效率

2. **冻结比例对比实验**
   - freeze_ratio=0.075 (仅Q/K)
   - freeze_ratio=0.25 (Q/K+部分FFN)
   - freeze_ratio=0.50 (Q/K+更多FFN)

### 🟡 中优先级 (短期)

3. **WikiText-103数据集验证**
   - 在更复杂的数据集上测试
   - 验证通用语言建模能力

4. **完成100K steps训练**
   - 当前进度: ~4%
   - 预计时间: 10-12小时

### 🟢 低优先级 (长期)

5. **多尺寸对比实验**
   - Tiny/Small/Medium/Large配置

6. **下游任务评估**
   - 文本生成、零样本学习等

---

## 实验命令速查

```bash
# 查看训练状态
./mlda-run.sh status

# 查看训练日志
./mlda-run.sh logs

# 实时查看GPT日志
ssh s125mdg43_10@gpu43.dynip.ntu.edu.sg 'tail -f ~/Orthogonal_ELM_Transformers/Train/logs/gpt_train.log'

# 实时查看OELM日志
ssh s125mdg43_10@gpu43.dynip.ntu.edu.sg 'tail -f ~/Orthogonal_ELM_Transformers/Train/logs/oelm_train.log'

# 停止训练
./mlda-run.sh kill-all
```

---

## 附录

### 模型检查点

| 模型 | 最佳Val Loss | 保存路径 |
|------|-------------|----------|
| GPT | 2.3812 | `models/checkpoints/gpt_medium512/best_model.pt` |
| OELM | 2.6056 | `models/checkpoints/oelm_medium512/best_model.pt` |

### 训练进度更新记录

#### 2026-02-06 17:30 - 第1次进度更新

**检查时间**: 2026-02-06 17:30 (训练时长: ~1小时)

**当前进度**:
| 模型 | Step | 最佳Val PPL | Val Loss | 进度百分比 |
|------|------|-------------|----------|-----------|
| GPT | 15,300 | **5.52** @ Step 15000 | 1.71 | 15.3% |
| OELM | 19,300 | **6.02** @ Step 19000 | 1.79 | 19.3% |

**与上次对比 (Step 10000/11900)**:
| 模型 | 上次Val PPL | 当前Val PPL | 改进 |
|------|-------------|-------------|------|
| GPT | 6.08 | 5.52 | -9.2% ✅ |
| OELM | 6.82 | 6.02 | -11.7% ✅ |

**性能差距变化**:
- 初始差距: 25% (10.82 vs 13.54)
- 当前差距: **9%** (5.52 vs 6.02) ✅ 大幅缩小

**关键发现**:
1. OELM训练速度比GPT快 **26%** (19300 vs 15300 steps)
2. 两者Val PPL都进入5-6区间，收敛良好
3. GPU温度正常 (80-82°C)，4卡100%运行
4. 预计剩余时间: 3-4小时

**状态**: ✅ 训练顺利，建议继续运行

---

#### 2026-02-06 18:00 - 第2次进度更新

**检查时间**: 2026-02-06 18:00 (训练时长: ~1.5小时)

**当前进度**:
| 模型 | Step | 最佳Val PPL | Val Loss | 进度百分比 |
|------|------|-------------|----------|-----------|
| GPT | 17,300 | **5.39** @ Step 17000 | 1.68 | 17.3% |
| OELM | 21,800 | **5.91** @ Step 21000 | 1.78 | 21.8% |

**与上次对比 (Step 15300/19300)**:
| 模型 | 上次Val PPL | 当前Val PPL | 改进 |
|------|-------------|-------------|------|
| GPT | 5.52 | 5.39 | -2.4% ✅ |
| OELM | 6.02 | 5.91 | -1.8% ✅ |

**性能差距变化**:
- 上次差距: 9% (5.52 vs 6.02)
- 当前差距: **9.6%** (5.39 vs 5.91) ⚠️ 基本持平

**关键发现**:
1. OELM训练速度比GPT快 **26%** (21800 vs 17300 steps)
2. GPT Val PPL进入5.3区间，收敛良好
3. OELM Val PPL进入5.9区间，追赶中
4. GPU运行稳定，4卡100%负载

**状态**: ✅ 训练稳定，建议继续运行至Step 25000

---

### 相关文档

| 文档 | 路径 |
|------|------|
| 实验报告 | `docs/experiment_report_medium512.md` |
| 训练脚本 | `mlda-run.sh` |
| OELM模型实现 | `models/modeling_oelm.py` |
| GPT模型实现 | `models/modeling_gpt.py` |

---

**日志创建时间**: 2026-02-06 16:45
**日志更新时间**: 2026-02-06 18:00
**下次更新**: 达到Step 25000或遇到重大问题时

---

*本日志由 Claude Code AI Assistant 协助生成*
*Generated with assistance from Claude Code AI Assistant*
