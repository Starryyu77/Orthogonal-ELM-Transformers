# 固定注意力模式与高效Transformer研究报告

## 摘要

本报告深入分析了固定注意力模式（Fixed Attention Patterns）在Transformer架构中的应用，以及各类高效注意力机制的设计原理。研究发现，大部分注意力头学习到的都是简单的位置模式，这启发了固定注意力在低资源场景下的成功应用。同时，Linear Attention、Performer等高效机制通过数学技巧将复杂度从$O(N^2)$降至$O(N)$。本报告还探讨了这些研究成果与"正交随机注意力"新范式之间的联系。

---

## 1. 固定注意力模式研究

### 1.1 "Fixed Encoder Self-Attention Patterns"论文核心发现

2020年，Raganato和Tiedemann等研究者发表了具有里程碑意义的论文《Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation》，揭示了Transformer编码器自注意力机制中一个令人惊讶的现象：

**核心发现**：大多数自注意力头学习到的仅仅是简单的位置模式，而非复杂的语义依赖关系。

#### 1.1.1 观察到的位置模式类型

研究者通过可视化分析发现，编码器中的注意力头主要呈现以下模式：

1. **当前token关注**（Current token）：注意力权重集中在位置$i$
2. **前一个token关注**（Previous token）：注意力权重集中在位置$i-1$
3. **后一个token关注**（Next token）：注意力权重集中在位置$i+1$
4. **左侧上下文关注**（Left context）：关注位置$0$到$i-2$的范围
5. **右侧上下文关注**（Right context）：关注位置$i+2$到$n$的范围
6. **句尾关注**（End of sentence）：关注整个序列的末尾
7. **句首关注**（Start of sentence）：关注整个序列的开头

这些模式完全基于位置信息，不需要任何可学习参数或外部知识。

#### 1.1.2 数学形式化定义

论文给出了固定注意力模式的数学定义。以前一个token模式和左侧上下文模式为例：

**前一个token模式**：

$$
\xi_{i,j}^{(2)} = 
\begin{cases}
1 & \text{if } j = i-1 \\
0 & \text{otherwise}
\end{cases}
$$

**左侧上下文模式**：

$$
\xi_{i,j}^{(4)} = 
\begin{cases}
f^{(4)}(j) & \text{if } j \leq i-2 \\
0 & \text{otherwise}
\end{cases}
$$

其中权重函数定义为：

$$
f^{(4)}(j) = \frac{(j+1)^3}{\sum_{j=0}^{i-2}(j+1)^3}
$$

这种基于位置的注意力分布反映了局部性原则的重要性。

### 1.2 为什么大部分注意力头学习简单位置模式

#### 1.2.1 局部性偏置（Locality Bias）

自然语言具有强烈的局部性特征：
- 相邻词语之间存在更强的语法和语义关联
- 局部上下文对词义消歧至关重要
- 长距离依赖相对稀疏

这种局部性使得模型倾向于学习关注邻近位置的简单模式。

#### 1.2.2 位置编码的影响

Transformer使用位置编码（Positional Encoding）注入位置信息：

$$
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d}) \\
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})
$$

位置编码使模型能够区分不同位置的token，但也使得注意力机制容易学习到基于位置的简单模式。

#### 1.2.3 多头注意力的冗余性

标准Transformer使用8个注意力头，但研究表明：
- 许多头学习到的模式高度相似
- 大量头可以被剪枝而不显著影响性能
- 不同层之间的模式也存在冗余

这种冗余性表明，模型可能"过度参数化"了注意力机制。

### 1.3 低资源场景下的性能提升

#### 1.3.1 实验设置

论文在多种数据规模下进行了实验：
- 标准场景：290万句对的WMT19德英翻译任务
- 低资源场景：不同规模的数据子集（5K-165K句对）
- 不同语言对：德语↔英语、越南语↔英语

#### 1.3.2 关键实验结果

| 模型配置 | 德→英BLEU | 英→德BLEU |
|---------|----------|----------|
| 8L (全可学习) | 40.5 | 38.2 |
| 7F_token+1L | 40.4 | 38.1 |
| 7F_word+1L | 40.3 | 38.0 |
| 1L (单头) | 39.1 | 37.0 |

**关键发现**：
1. 在标准场景下，固定注意力模型与全可学习模型性能相当
2. 在低资源场景下，固定注意力模型**提升BLEU达3个点**
3. 固定模式始终优于单头可学习模型

#### 1.3.3 低资源提升的原因分析

固定注意力在低资源场景下表现更好的原因：

1. **减少过拟合**：固定模式作为强先验，减少了可训练参数数量
2. **更好的泛化**：基于位置的简单模式具有更好的泛化能力
3. **训练稳定性**：固定的注意力分布提供了更稳定的梯度信号
4. **参数效率**：将模型容量集中在更需要学习的地方

---

## 2. 高效注意力机制

### 2.1 Linear Attention：从$O(N^2)$到$O(N)$

#### 2.1.1 标准注意力的复杂度问题

标准Transformer自注意力的计算：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

复杂度分析：
- $Q, K, V \in \mathbb{R}^{N \times d}$
- $QK^T \in \mathbb{R}^{N \times N}$需要$O(N^2d)$时间和$O(N^2)$空间
- 对于长序列，这成为严重的瓶颈

#### 2.1.2 Linear Attention的核心思想

Katharopoulos等人（2020）提出的Linear Attention通过**核技巧**（Kernel Trick）解决复杂度问题。

**关键洞察**：将softmax注意力重写为核特征映射的内积：

$$
\text{Attention}(Q, K, V) = \phi(Q)\phi(K)^T V
$$

其中$\phi(\cdot)$是特征映射函数。

#### 2.1.3 矩阵乘法的结合律

利用矩阵乘法的结合律，改变计算顺序：

$$
\underbrace{(\phi(Q)\phi(K)^T)}_{O(N^2d)} V \rightarrow \phi(Q)\underbrace{(\phi(K)^T V)}_{O(Nd^2)}
$$

**复杂度对比**：

| 操作 | 标准Attention | Linear Attention |
|-----|--------------|------------------|
| 计算$QK^T$ | $O(N^2d)$ | - |
| 计算$\phi(K)^TV$ | - | $O(Nd^2)$ |
| 存储注意力矩阵 | $O(N^2)$ | $O(d^2)$ |
| 总时间复杂度 | $O(N^2d)$ | $O(Nd^2)$ |

当$d \ll N$时，Linear Attention显著更高效。

#### 2.1.4 常用核函数

不同的Linear Attention变体使用不同的特征映射：

1. **ELU核**（Katharopoulos et al., 2020）：
   $$
   \phi(x) = \text{elu}(x) + 1
   $$

2. **余弦核**（Qin et al., 2022）：
   $$
   \phi(x) = \cos(x)
   $$

3. **多项式核**：
   $$
   \phi(x) = [1, x, x^2, ...]
   $$

### 2.2 Performer：正交随机特征近似

#### 2.2.1 FAVOR+算法

Choromanski等人（2020）提出的Performer使用**Fast Attention Via positive Orthogonal Random features (FAVOR+)**机制。

**核心思想**：使用随机特征映射近似softmax核：

$$
\exp(x^Ty) \approx \mathbb{E}_{\omega \sim \mathcal{N}(0, I_d)}[\exp(\omega^T x - \frac{1}{2}\|x\|^2) \exp(\omega^T y - \frac{1}{2}\|y\|^2)]
$$

#### 2.2.2 正交随机特征

FAVOR+使用正交随机矩阵$\Omega \in \mathbb{R}^{m \times d}$：

$$
\phi(x) = \exp(\Omega x - \frac{1}{2}\|x\|^2)
$$

**正交性的优势**：
- 减少随机特征的方差
- 提供更稳定的近似
- 保证无偏性

#### 2.2.3 复杂度分析

Performer的复杂度：
- **时间复杂度**：$O(Nmd)$，其中$m$是随机特征数量
- **空间复杂度**：$O(Nm)$
- 当$m \ll N$时，达到线性复杂度

#### 2.2.4 FAVOR+的变体

| 方法 | 随机特征 | 方差特性 |
|-----|---------|---------|
| TrigRF | 正弦/余弦 | 高，不稳定 |
| FAVOR+ | 正指数 | 稳定但方差较高 |
| FAVOR++ | GERF | 方差降低$e^2$-$e^3$倍 |
| FAVOR# | SDERF/ADERF | 方差降低达$e^{10}$倍 |

### 2.3 Sparse Transformer的固定注意力模式

#### 2.3.1 分解注意力策略

Sparse Transformer（Child et al., 2019）提出了**分解注意力**（Factorized Attention）策略，将注意力分解为多个固定模式。

**两种主要模式**：

1. **Strided Attention（步进注意力）**：
   - 第一个头关注前$l$个位置
   - 第二个头关注每隔$l$个位置
   - $l \approx \sqrt{n}$

2. **Fixed Attention（固定注意力）**：
   - 特定单元总结先前位置
   - 将信息传播到所有未来单元

#### 2.3.2 Strided Attention的数学定义

$$
A_i^{(1)} = \{t, t+1, ..., i\} \quad \text{for } t = \max(0, i-l) \\
A_i^{(2)} = \{j : (i-j) \mod l = 0\}
$$

**复杂度**：$O(N\sqrt{N})$

#### 2.3.3 信息传播机制

Strided Attention通过**两跳连接**实现全局信息传播：
- 第1层：每个位置连接到最近的hub和它自己
- 第2层：hubs之间可以相互连接
- 结果：任意两个位置在两层内可达

**可视化**：
```
Layer 1:  [每个位置 → 最近的hub + 自己]
Layer 2:  [hubs之间互相连接]
结果:     [任意位置 → 任意位置]
```

#### 2.3.4 其他稀疏模式

1. **Local Attention（局部注意力）**：
   - 每个位置只关注窗口内的邻居
   - 复杂度：$O(N \cdot w)$，$w$为窗口大小

2. **Block-Sparse Attention（块稀疏注意力）**：
   - 将序列分成块，在块级别定义注意力
   - 硬件友好，适合GPU并行

3. **Global+Local组合**：
   - 部分token可以全局关注
   - 其他token只能局部关注

---

## 3. 数学分析

### 3.1 标准Attention的$O(N^2)$复杂度

#### 3.1.1 计算图分析

标准自注意力的计算步骤：

```
输入: X ∈ ℝ^(N×d)

Step 1: Q = XW_Q, K = XW_K, V = XW_V
        → 3次矩阵乘法: O(Nd^2)

Step 2: S = QK^T / √d_k
        → 1次矩阵乘法: O(N^2d)

Step 3: A = softmax(S)
        → 逐行softmax: O(N^2)

Step 4: O = AV
        → 1次矩阵乘法: O(N^2d)

输出: O ∈ ℝ^(N×d)
```

#### 3.1.2 复杂度汇总

| 步骤 | 操作 | 时间复杂度 | 空间复杂度 |
|-----|------|----------|----------|
| 1 | 线性投影 | $O(Nd^2)$ | $O(Nd)$ |
| 2 | 计算相似度 | $O(N^2d)$ | $O(N^2)$ |
| 3 | Softmax归一化 | $O(N^2)$ | $O(N^2)$ |
| 4 | 加权聚合 | $O(N^2d)$ | $O(Nd)$ |
| **总计** | | **$O(N^2d)$** | **$O(N^2)$** |

### 3.2 Linear Attention的核技巧

#### 3.2.1 核方法基础

Mercer定理：任何半正定核函数$k(x, y)$都可以表示为特征映射的内积：

$$
k(x, y) = \phi(x)^T \phi(y)
$$

#### 3.2.2 Softmax作为核函数

Softmax注意力可以看作使用指数核：

$$
\text{softmax}(QK^T)V = D^{-1}\exp(QK^T)V
$$

其中$D$是对角归一化矩阵。

#### 3.2.3 核技巧的应用

将指数核分解：

$$
\exp(q_i^T k_j) = \phi(q_i)^T \phi(k_j)
$$

对于指数核，特征映射可以是：

$$
\phi(x) = \exp(\Omega x - \frac{1}{2}\|x\|^2)
$$

其中$\Omega$是随机投影矩阵。

#### 3.2.4 矩阵结合律的详细推导

原始形式：

$$
O_i = \frac{\sum_{j=1}^N \exp(q_i^T k_j) v_j}{\sum_{j=1}^N \exp(q_i^T k_j)}
$$

使用核技巧：

$$
O_i = \frac{\sum_{j=1}^N \phi(q_i)^T \phi(k_j) v_j}{\sum_{j=1}^N \phi(q_i)^T \phi(k_j)} = \frac{\phi(q_i)^T \sum_{j=1}^N \phi(k_j) v_j^T}{\phi(q_i)^T \sum_{j=1}^N \phi(k_j)}
$$

定义累积项：

$$
S = \sum_{j=1}^N \phi(k_j) v_j^T \in \mathbb{R}^{m \times d} \\
z = \sum_{j=1}^N \phi(k_j) \in \mathbb{R}^{m}
$$

则：

$$
O_i = \frac{\phi(q_i)^T S}{\phi(q_i)^T z}
$$

**复杂度对比**：
- 计算$S$和$z$：$O(Nmd)$
- 计算每个$O_i$：$O(md)$
- 总计：$O(Nmd)$，当$m \ll N$时为$O(N)$

### 3.3 Performer的FAVOR+算法详解

#### 3.3.1 无偏估计证明

**定理**：对于$\omega \sim \mathcal{N}(0, I_d)$，有：

$$
\mathbb{E}_{\omega}[\exp(\omega^T x - \frac{1}{2}\|x\|^2) \exp(\omega^T y - \frac{1}{2}\|y\|^2)] = \exp(x^T y)
$$

**证明**：

令$z = \omega^T (x + y)$，则$z \sim \mathcal{N}(0, \|x+y\|^2)$

$$
\mathbb{E}[\exp(z)] = \exp(\frac{1}{2}\|x+y\|^2)
$$

因此：

$$
\mathbb{E}[\exp(\omega^T x - \frac{1}{2}\|x\|^2) \exp(\omega^T y - \frac{1}{2}\|y\|^2)] \\
= \exp(-\frac{1}{2}\|x\|^2 - \frac{1}{2}\|y\|^2) \cdot \exp(\frac{1}{2}\|x+y\|^2) \\
= \exp(-\frac{1}{2}\|x\|^2 - \frac{1}{2}\|y\|^2 + \frac{1}{2}\|x\|^2 + x^Ty + \frac{1}{2}\|y\|^2) \\
= \exp(x^T y)
$$

#### 3.3.2 正交随机特征的优势

标准随机特征：$\omega_1, ..., \omega_m \sim \mathcal{N}(0, I_d)$独立采样

正交随机特征：
1. 从$\mathcal{N}(0, I_d)$采样$m$个向量
2. 使用Gram-Schmidt正交化
3. 结果：$\Omega^T \Omega = I_m$

**方差减少**：正交化减少了随机特征之间的相关性，降低了估计方差。

#### 3.3.3 FAVOR+的完整算法

```
算法: FAVOR+ Attention

输入: Q, K, V ∈ ℝ^(N×d), 特征数m
输出: O ∈ ℝ^(N×d)

1. 生成正交随机矩阵 Ω ∈ ℝ^(m×d)
2. 计算特征映射:
   φ(Q) = exp(QΩ^T - 0.5||Q||^2) ∈ ℝ^(N×m)
   φ(K) = exp(KΩ^T - 0.5||K||^2) ∈ ℝ^(N×m)
3. 计算累积项:
   S = φ(K)^T V ∈ ℝ^(m×d)
   z = sum(φ(K), dim=0) ∈ ℝ^m
4. 对每个位置i:
   O_i = φ(Q_i)^T S / (φ(Q_i)^T z)
5. 返回 O
```

---

## 4. 与本研究（正交随机注意力）的联系

### 4.1 固定注意力模式的成功经验

#### 4.1.1 关键经验总结

1. **位置模式足够强大**：简单的位置注意力模式已经能够捕获大部分有用的信息
2. **减少可训练参数**：固定模式减少了模型复杂度，降低过拟合风险
3. **低资源场景受益**：固定模式作为强先验，在低资源下表现更好
4. **保留一个可学习头**：保留部分灵活性对复杂语义任务有益

#### 4.1.2 对本研究的启示

这些经验支持了"正交随机注意力"的核心假设：
- 注意力投影矩阵可能不需要完全训练
- 固定的、结构化的投影可能足够有效
- 正交约束提供了良好的归纳偏置

### 4.2 正交随机注意力与固定模式的区别

#### 4.2.1 核心区别

| 特性 | 固定注意力模式 | 正交随机注意力 |
|-----|--------------|--------------|
| 固定对象 | 注意力权重分布 | Q/K投影矩阵 |
| 是否随机 | 否（确定性模式） | 是（随机初始化） |
| 正交性 | 不涉及 | 核心特性 |
| 表达能力 | 固定模式 | 保留表达能力 |
| 训练过程 | 完全冻结 | 全程冻结 |

#### 4.2.2 正交随机注意力的优势

1. **保留表达能力**：
   - 固定模式将注意力限制在预定义形状
   - 正交随机投影保留了完整的表达能力
   - 随机投影可以近似任意线性变换

2. **Johnson-Lindenstrauss性质**：
   随机正交投影保持向量间的相对距离：

   $$
   (1-\epsilon)\|x-y\|^2 \leq \|\Omega x - \Omega y\|^2 \leq (1+\epsilon)\|x-y\|^2
   $$

3. **更好的泛化**：
   - 正交约束减少了假设空间
   - 随机初始化提供了良好的起点
   - 避免了训练过程中的过拟合

4. **计算效率**：
   - 冻结投影矩阵减少了可训练参数
   - 正交矩阵有高效的乘法算法
   - 可以预计算和缓存

### 4.3 为什么正交随机投影更有优势

#### 4.3.1 理论优势

**定理**（Johnson-Lindenstrauss）：对于任意$\epsilon \in (0, 1)$和整数$n$，设$k$为满足以下条件的最小整数：

$$
k \geq \frac{4 \log n}{\epsilon^2/2 - \epsilon^3/3}
$$

则对于$\mathbb{R}^d$中的任意$n$个点组成的集合$P$，存在映射$f: \mathbb{R}^d \rightarrow \mathbb{R}^k$，使得对所有$u, v \in P$：

$$
(1-\epsilon)\|u-v\|^2 \leq \|f(u)-f(v)\|^2 \leq (1+\epsilon)\|u-v\|^2
$$

随机正交投影以高概率满足此性质。

#### 4.3.2 实践优势

1. **训练稳定性**：
   - 固定投影避免了训练过程中的梯度爆炸/消失
   - 正交性保持了特征尺度

2. **内存效率**：
   - 不需要存储投影矩阵的梯度
   - 可以共享投影矩阵 across layers

3. **推断效率**：
   - 投影矩阵可以预计算
   - 正交矩阵的乘法可以通过快速算法实现

#### 4.3.3 与Performer的联系

Performer的FAVOR+算法也使用了正交随机特征，但用途不同：

| 方面 | Performer FAVOR+ | 正交随机注意力 |
|-----|-----------------|--------------|
| 正交矩阵用途 | 近似softmax核 | 作为Q/K投影矩阵 |
| 是否学习 | 冻结 | 冻结 |
| 主要目标 | 降低复杂度 | 减少可训练参数 |
| 注意力计算 | 使用随机特征 | 标准注意力形式 |

两者都利用了正交随机矩阵的良好性质，但应用场景不同。

### 4.4 综合比较

```
                    标准Transformer    固定注意力    Linear Attention    Performer    正交随机注意力
                    ─────────────────────────────────────────────────────────────────────────────
注意力权重           学习得到            预定义        核函数计算          随机特征近似    学习得到
Q/K/V投影            学习得到            学习得到      学习得到           学习得到       正交随机+冻结
复杂度               O(N²d)             O(N²d)       O(Nd²)             O(Nmd)        O(N²d)
可训练参数           多                 较少          中等               中等          较少
低资源性能           基准               +3 BLEU      接近基准            接近基准       待验证
表达能力             完整               受限          完整               近似完整       完整
```

---

## 5. 结论与展望

### 5.1 主要发现

1. **固定注意力模式的有效性**：大部分注意力头学习到的都是简单的位置模式，固定这些模式可以在低资源场景下提升BLEU达3个点。

2. **高效注意力机制的多样性**：Linear Attention、Performer、Sparse Transformer等从不同角度解决了$O(N^2)$复杂度问题。

3. **正交随机投影的潜力**：正交随机矩阵在保持表达能力的同时，提供了良好的归纳偏置和计算效率。

### 5.2 对本研究的启示

1. **理论支持**：固定注意力模式的成功为正交随机注意力提供了理论支持。

2. **设计借鉴**：可以借鉴Linear Attention的核技巧和Performer的正交随机特征设计。

3. **实验验证**：需要在低资源场景下验证正交随机注意力的性能优势。

### 5.3 未来研究方向

1. **理论分析**：深入分析正交随机投影对注意力机制表达能力的影响。

2. **混合策略**：探索部分头使用正交随机投影、部分头可学习的混合策略。

3. **与高效机制结合**：将正交随机注意力与Linear Attention或Sparse Attention结合。

4. **跨任务验证**：在更多NLP任务上验证正交随机注意力的有效性。

---

## 参考文献

1. Raganato, A., & Tiedemann, J. (2020). Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation. EMNLP 2020 Findings.

2. Katharopoulos, A., et al. (2020). Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. ICML 2020.

3. Choromanski, K., et al. (2020). Rethinking Attention with Performers. ICLR 2021.

4. Child, R., et al. (2019). Generating Long Sequences with Sparse Transformers. arXiv:1904.10509.

5. Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS 2017.

6. Voita, E., et al. (2019). Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. ACL 2019.

7. Kovaleva, O., et al. (2019). Revealing the Dark Secrets of BERT. EMNLP 2019.

8. Shen, Z., et al. (2021). Efficient Attention: A Study of Linear Time Transformers. ICLR 2021.

---

*报告生成时间：2025年*
*作者：AI研究助手*
