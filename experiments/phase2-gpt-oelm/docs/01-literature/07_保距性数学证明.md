# 正交矩阵的等距性质及其在随机投影中的应用

## 完整数学推导文档

---

## 1. 正交矩阵的基本性质

### 1.1 定义

**定义 1.1（正交矩阵）**：设 $Q \in \mathbb{R}^{n \times n}$ 是一个方阵，如果满足：

$$Q^T Q = Q Q^T = I$$

则称 $Q$ 为**正交矩阵**（Orthogonal Matrix）。

等价地，正交矩阵的逆等于其转置：

$$Q^{-1} = Q^T$$

### 1.2 行列式性质

**定理 1.1**：设 $Q$ 是正交矩阵，则 $\det(Q) = \pm 1$。

**证明**：

由正交矩阵的定义 $Q^T Q = I$，两边取行列式：

$$\det(Q^T Q) = \det(I)$$

利用行列式性质 $\det(AB) = \det(A)\det(B)$ 和 $\det(Q^T) = \det(Q)$：

$$\det(Q^T) \cdot \det(Q) = 1$$

$$\det(Q) \cdot \det(Q) = 1$$

$$[\det(Q)]^2 = 1$$

因此：

$$\det(Q) = \pm 1$$

**证毕。** $\square$

### 1.3 特征值性质

**定理 1.2**：设 $Q$ 是正交矩阵，$\lambda$ 是 $Q$ 的任意特征值，则 $|\lambda| = 1$。

**证明**：

设 $\lambda$ 是 $Q$ 的特征值，$v \neq 0$ 是对应的特征向量，满足：

$$Qv = \lambda v$$

考虑 $v^* Q^T Q v$，其中 $v^*$ 表示 $v$ 的共轭转置：

$$v^* Q^T Q v = v^* I v = v^* v = \|v\|^2$$

另一方面：

$$v^* Q^T Q v = (Qv)^* (Qv) = (\lambda v)^* (\lambda v) = \bar{\lambda} \lambda \cdot v^* v = |\lambda|^2 \|v\|^2$$

因此：

$$|\lambda|^2 \|v\|^2 = \|v\|^2$$

由于 $v \neq 0$，有 $\|v\|^2 > 0$，故：

$$|\lambda|^2 = 1 \implies |\lambda| = 1$$

**证毕。** $\square$

---

## 2. 等距性质的严格证明

### 2.1 范数保持性

**定理 2.1（等距性质 - Isometry）**：设 $Q \in \mathbb{R}^{n \times n}$ 是正交矩阵，则对任意向量 $x \in \mathbb{R}^n$：

$$\|Qx\| = \|x\|$$

**证明**：

考虑 $\|Qx\|^2$：

$$\|Qx\|^2 = (Qx)^T (Qx) = x^T Q^T Q x = x^T I x = x^T x = \|x\|^2$$

两边开平方：

$$\|Qx\| = \|x\|$$

**证毕。** $\square$

### 2.2 内积保持性

**定理 2.2（内积保持）**：设 $Q$ 是正交矩阵，则对任意 $x, y \in \mathbb{R}^n$：

$$(Qx)^T (Qy) = x^T y$$

**证明**：

$$(Qx)^T (Qy) = x^T Q^T Q y = x^T I y = x^T y$$

**证毕。** $\square$

### 2.3 角度保持性

**定理 2.3（角度保持）**：设 $Q$ 是正交矩阵，则对任意非零向量 $x, y \in \mathbb{R}^n$，正交变换保持向量间夹角不变：

$$\cos \theta_{Qx, Qy} = \cos \theta_{x, y}$$

其中 $\theta_{x,y}$ 表示向量 $x$ 和 $y$ 之间的夹角。

**证明**：

向量夹角的余弦定义为：

$$\cos \theta_{x,y} = \frac{x^T y}{\|x\| \|y\|}$$

对于变换后的向量 $Qx$ 和 $Qy$：

$$\cos \theta_{Qx, Qy} = \frac{(Qx)^T (Qy)}{\|Qx\| \|Qy\|}$$

利用定理 2.1 和定理 2.2：

$$= \frac{x^T y}{\|x\| \|y\|} = \cos \theta_{x,y}$$

因此，正交变换保持向量间的夹角不变。

**证毕。** $\square$

### 2.4 欧氏距离保持性

**推论 2.1**：正交变换保持欧氏距离：

$$\|Qx - Qy\| = \|x - y\|$$

**证明**：

$$\|Qx - Qy\|^2 = \|Q(x - y)\|^2 = \|x - y\|^2$$

其中最后一步利用了定理 2.1。

**证毕。** $\square$

---

## 3. Johnson-Lindenstrauss引理

### 3.1 引理陈述

**定理 3.1（Johnson-Lindenstrauss引理）**：

设 $X = \{x_1, x_2, \ldots, x_n\} \subset \mathbb{R}^d$ 是 $n$ 个点组成的集合，$\varepsilon \in (0, 1)$ 是给定的失真参数。则存在一个映射 $f: \mathbb{R}^d \to \mathbb{R}^k$，其中：

$$k \geq \frac{4 \ln n}{\varepsilon^2/2 - \varepsilon^3/3}$$

使得对所有 $x_i, x_j \in X$：

$$(1 - \varepsilon) \|x_i - x_j\|^2 \leq \|f(x_i) - f(x_j)\|^2 \leq (1 + \varepsilon) \|x_i - x_j\|^2$$

### 3.2 随机投影构造

最常用的构造方法是**随机投影**：

$$f(x) = \frac{1}{\sqrt{k}} R x$$

其中 $R \in \mathbb{R}^{k \times d}$ 是随机矩阵，其元素独立同分布于 $\mathcal{N}(0, 1)$。

### 3.3 证明概要

**引理 3.1（高斯随机变量的集中性）**：设 $Z_1, Z_2, \ldots, Z_k \overset{iid}{\sim} \mathcal{N}(0, 1)$，则：

$$\Pr\left[\left|\frac{1}{k}\sum_{i=1}^k Z_i^2 - 1\right| \geq \varepsilon\right] \leq 2e^{-k\varepsilon^2/8}$$

**主证明**：

**步骤 1**：固定一对点 $x_i, x_j$，设 $u = x_i - x_j$。

定义投影后的距离：

$$\|f(x_i) - f(x_j)\|^2 = \left\|\frac{1}{\sqrt{k}} R u\right\|^2 = \frac{1}{k} \sum_{l=1}^k (R_l^T u)^2$$

其中 $R_l$ 是 $R$ 的第 $l$ 行。

**步骤 2**：分析 $(R_l^T u)^2$ 的分布。

由于 $R_l \sim \mathcal{N}(0, I_d)$，有：

$$R_l^T u = \sum_{j=1}^d R_{lj} u_j \sim \mathcal{N}(0, \|u\|^2)$$

因此：

$$\frac{(R_l^T u)^2}{\|u\|^2} \sim \chi^2_1$$

**步骤 3**：利用 $\chi^2$ 分布的集中不等式。

设 $Y_l = \frac{(R_l^T u)^2}{\|u\|^2}$，则 $Y_l \overset{iid}{\sim} \chi^2_1$，且：

$$\frac{\|f(x_i) - f(x_j)\|^2}{\|x_i - x_j\|^2} = \frac{1}{k}\sum_{l=1}^k Y_l$$

利用 $\chi^2$ 分布的尾界：

$$\Pr\left[\left|\frac{1}{k}\sum_{l=1}^k Y_l - 1\right| \geq \varepsilon\right] \leq 2e^{-k\varepsilon^2/8}$$

**步骤 4**：对所有点对取并集界（Union Bound）。

共有 $\binom{n}{2} < n^2/2$ 对点，因此：

$$\Pr[\exists i,j: \text{失真} > \varepsilon] \leq n^2 \cdot 2e^{-k\varepsilon^2/8}$$

为使该概率小于 1，需要：

$$n^2 \cdot 2e^{-k\varepsilon^2/8} < 1$$

$$2\ln n + \ln 2 < \frac{k\varepsilon^2}{8}$$

$$k > \frac{8(2\ln n + \ln 2)}{\varepsilon^2} \approx \frac{16\ln n}{\varepsilon^2}$$

更精细的分析给出：

$$k \geq \frac{4 \ln n}{\varepsilon^2/2 - \varepsilon^3/3}$$

**证毕。** $\square$

### 3.4 维数下界的直观解释

JL引理的核心洞察是：

$$k = O\left(\frac{\ln n}{\varepsilon^2}\right)$$

这意味着：
- 所需维度仅与点数的对数成正比
- 与原始维度 $d$ 无关
- 失真参数 $\varepsilon$ 越小，所需维度越高

---

## 4. 正交随机投影 vs 高斯随机投影

### 4.1 高斯随机矩阵的保距性

**定义 4.1（高斯随机矩阵）**：$G \in \mathbb{R}^{k \times d}$，其中 $G_{ij} \overset{iid}{\sim} \mathcal{N}(0, 1/k)$。

**定理 4.1（高斯投影的保距性）**：设 $G$ 是高斯随机矩阵，则对任意固定向量 $x \in \mathbb{R}^d$：

$$\mathbb{E}[\|Gx\|^2] = \|x\|^2$$

**证明**：

$$\mathbb{E}[\|Gx\|^2] = \mathbb{E}[x^T G^T G x] = x^T \mathbb{E}[G^T G] x$$

计算 $(G^T G)_{ij} = \sum_{l=1}^k G_{li} G_{lj}$：

- 当 $i = j$：$\mathbb{E}[(G^T G)_{ii}] = \sum_{l=1}^k \mathbb{E}[G_{li}^2] = k \cdot \frac{1}{k} = 1$
- 当 $i \neq j$：$\mathbb{E}[(G^T G)_{ij}] = \sum_{l=1}^k \mathbb{E}[G_{li}]\mathbb{E}[G_{lj}] = 0$

因此 $\mathbb{E}[G^T G] = I$，故：

$$\mathbb{E}[\|Gx\|^2] = x^T I x = \|x\|^2$$

**证毕。** $\square$

### 4.2 正交随机矩阵

**定义 4.2（正交随机矩阵）**：$Q \in \mathbb{R}^{k \times d}$（$k \leq d$）是正交随机矩阵，如果：

1. $Q Q^T = I_k$（行正交）
2. $Q$ 的行向量在 $\mathbb{R}^d$ 的单位球面上均匀分布

**构造方法（Haar测度采样）**：

1. 生成 $d \times d$ 高斯随机矩阵 $G$，$G_{ij} \overset{iid}{\sim} \mathcal{N}(0, 1)$
2. 进行QR分解：$G = QR$
3. 取 $Q$ 的前 $k$ 行

### 4.3 谱半径对比分析

**定义 4.3（谱半径）**：矩阵 $A$ 的谱半径定义为：

$$\rho(A) = \max_{i} |\lambda_i(A)|$$

其中 $\lambda_i(A)$ 是 $A$ 的特征值。

**定理 4.2（正交矩阵的谱半径）**：设 $Q$ 是正交矩阵，则：

$$\rho(Q) = 1$$

**证明**：由定理 1.2，正交矩阵的所有特征值满足 $|\lambda| = 1$，故 $\rho(Q) = 1$。$\square$

**定理 4.3（高斯随机矩阵的谱半径）**：设 $G \in \mathbb{R}^{k \times d}$ 是高斯随机矩阵，$k \leq d$，则渐近地：

$$\rho(G^T G) \approx \left(\frac{\sqrt{d} + \sqrt{k}}{\sqrt{k}}\right)^2 = \left(1 + \sqrt{\frac{d}{k}}\right)^2$$

**对比分析**：

| 性质 | 正交随机矩阵 | 高斯随机矩阵 |
|------|-------------|-------------|
| 谱半径 | $\rho = 1$（精确） | $\rho \approx (1 + \sqrt{d/k})^2$（随机） |
| 条件数 | $\kappa = 1$ | $\kappa \approx \frac{(1+\sqrt{d/k})^2}{(1-\sqrt{d/k})^2}$ |
| 范数保持 | 精确保持 | 期望保持，有方差 |
| 数值稳定性 | 最优 | 可能不稳定 |

### 4.4 正交随机投影的优势

**优势 1：精确的等距性**

对于正交随机矩阵 $Q$：

$$\|Qx\|^2 = x^T Q^T Q x = x^T P x$$

其中 $P = Q^T Q$ 是投影矩阵。当 $k = d$ 时，$P = I$，精确保持范数。

**优势 2：更好的集中性**

**定理 4.4**：设 $Q$ 是正交随机矩阵，$x \in \mathbb{R}^d$，$\|x\| = 1$，则：

$$\Pr\left[\left|\|Qx\|^2 - \frac{k}{d}\right| \geq \varepsilon \cdot \frac{k}{d}\right] \leq 2e^{-k\varepsilon^2/8}$$

相比高斯投影，正交投影的方差更小。

**优势 3：数值稳定性**

正交矩阵的条件数 $\kappa(Q) = 1$，而高斯矩阵的条件数随 $d/k$ 增大而恶化。

---

## 5. 在Attention机制中的应用

### 5.1 注意力机制回顾

标准注意力机制定义为：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

其中 $Q, K, V \in \mathbb{R}^{n \times d_k}$ 分别是查询、键、值矩阵。

### 5.2 QK^T的保距性分析

**定理 5.1**：设 $W_Q, W_K \in \mathbb{R}^{d_k \times d}$ 是正交矩阵（$d_k \leq d$），$X \in \mathbb{R}^{n \times d}$ 是输入，则：

$$Q = X W_Q^T, \quad K = X W_K^T$$

**分析**：考虑 $QK^T$ 的 $(i,j)$ 元素：

$$(QK^T)_{ij} = (X W_Q^T)_i (X W_K^T)_j^T = x_i^T W_Q^T W_K x_j$$

**特殊情况**：当 $W_Q = W_K = W$ 且 $W$ 正交时：

$$(QK^T)_{ij} = x_i^T W^T W x_j = x_i^T P x_j$$

其中 $P = W^T W$ 是投影矩阵。若 $W$ 是方阵，则 $P = I$，此时：

$$(QK^T)_{ij} = x_i^T x_j$$

即 $QK^T = XX^T$，完全保持原始内积结构。

### 5.3 Token表示的距离保持

**定义 5.1（Token距离）**：对于Token表示 $t_i, t_j \in \mathbb{R}^d$，定义其距离为：

$$d(t_i, t_j) = \|t_i - t_j\|$$

**定理 5.2（正交投影保持Token距离）**：设 $W$ 是正交矩阵，$t_i' = W t_i$，则：

$$d(t_i', t_j') = d(t_i, t_j)$$

**证明**：由定理 2.1，正交变换保持范数，因此保持距离。$\square$

**意义**：在Transformer中，正交投影确保：
1. Token间的相对距离不变
2. 语义相似性得以保持
3. 注意力分数反映真实的Token关系

### 5.4 对注意力分布的影响

**分析 5.1（注意力权重）**：注意力权重定义为：

$$\alpha_{ij} = \frac{\exp(q_i^T k_j / \sqrt{d_k})}{\sum_{l=1}^n \exp(q_i^T k_l / \sqrt{d_k})}$$

当 $W_Q$ 和 $W_K$ 都是正交矩阵时：

$$q_i^T k_j = (W_Q x_i)^T (W_K x_j) = x_i^T W_Q^T W_K x_j$$

**定理 5.3**：若 $W_Q = W_K = W$ 且 $W$ 正交，则：

$$q_i^T k_j = x_i^T x_j$$

这意味着注意力分数完全由输入Token的内积决定，不受投影影响。

**推论 5.1**：正交投影下，注意力分布 $\{\alpha_{ij}\}$ 仅依赖于输入的原始几何结构。

### 5.5 正交随机注意力的优势

**优势总结**：

1. **稳定性**：正交投影保证数值稳定性，避免梯度爆炸/消失

2. **保距性**：Token间的距离和角度关系精确保持

3. **可解释性**：注意力分数反映真实的语义相似性

4. **效率**：正交矩阵可以通过结构化方式（如Householder变换）高效计算

---

## 6. 总结与核心结论

### 6.1 核心定理回顾

| 定理 | 内容 | 意义 |
|------|------|------|
| 定理 2.1 | $\|Qx\| = \|x\|$ | 正交变换保持范数 |
| 定理 2.2 | $(Qx)^T(Qy) = x^T y$ | 正交变换保持内积 |
| 定理 2.3 | 角度保持 | 正交变换保持几何结构 |
| JL引理 | 随机投影保距 | 高维数据可降维保持距离 |
| 定理 5.2 | Token距离保持 | 正交注意力保持语义关系 |

### 6.2 正交性对Transformer的重要性

1. **训练稳定性**：正交权重矩阵条件数为1，避免梯度问题

2. **表示一致性**：不同层间的Token距离保持一致

3. **注意力质量**：注意力分数准确反映Token相似性

4. **泛化能力**：保距性确保模型学到鲁棒的特征表示

### 6.3 数学符号速查

| 符号 | 含义 |
|------|------|
| $Q$ | 正交矩阵，$Q^T Q = I$ |
| $\|x\|$ | 欧氏范数，$\sqrt{x^T x}$ |
| $\rho(A)$ | 矩阵 $A$ 的谱半径 |
| $\kappa(A)$ | 矩阵 $A$ 的条件数 |
| $\varepsilon$ | JL引理中的失真参数 |
| $k$ | 投影后的维度 |
| $d$ | 原始维度 |

---

## 参考文献

1. Johnson, W. B., & Lindenstrauss, J. (1984). Extensions of Lipschitz mappings into a Hilbert space. *Contemporary Mathematics*, 26, 189-206.

2. Dasgupta, S., & Gupta, A. (2003). An elementary proof of a theorem of Johnson and Lindenstrauss. *Random Structures & Algorithms*, 22(1), 60-65.

3. Ailon, N., & Chazelle, B. (2006). Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform. *STOC*, 557-563.

4. Vaswani, A., et al. (2017). Attention is all you need. *NeurIPS*, 5998-6008.

---

*文档生成时间：数学推导完成*
*适用于：正交随机注意力Transformer研究*
