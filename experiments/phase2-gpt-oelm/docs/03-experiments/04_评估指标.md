# 正交随机注意力实验评估指标体系

## 目录
1. [主要评估指标](#1-主要评估指标)
2. [效率指标](#2-效率指标)
3. [稳定性指标](#3-稳定性指标)
4. [统计显著性检验](#4-统计显著性检验)
5. [可视化方案](#5-可视化方案)
6. [指标对比表](#6-指标对比表)
7. [实施建议](#7-实施建议)

---

## 1. 主要评估指标

### 1.1 Perplexity（困惑度）

困惑度是语言模型最核心的评估指标，衡量模型预测下一个词的能力。

#### 数学定义

$$\text{PPL} = \exp\left( -\frac{1}{N} \sum_{i=1}^{N} \log P(x_i | x_{<i}) \right)$$

其中：
- $N$：序列中的token总数
- $P(x_i | x_{<i})$：模型预测第$i$个token的条件概率
- 对数似然：$\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} \log P(x_i | x_{<i})$

#### 变体形式

**字节级困惑度（Bytes Per Character）**：
$$\text{BPB} = -\frac{1}{\sum_{i} \text{len}(x_i)} \sum_{i=1}^{N} \log_2 P(x_i | x_{<i})$$

**词级困惑度（Word-level PPL）**：
$$\text{Word-PPL} = \exp\left( -\frac{1}{M} \sum_{j=1}^{M} \log P(w_j | w_{<j}) \right)$$

其中$M$为词的数量。

#### 评估维度

| 维度 | 说明 | 适用场景 |
|------|------|----------|
| 训练集PPL | 模型拟合能力 | 检测过拟合 |
| 验证集PPL | 泛化能力 | 模型选择 |
| 测试集PPL | 最终性能 | 论文报告 |
| 领域特定PPL | 领域适应性 | 迁移学习 |

---

### 1.2 Loss曲线（训练/验证）

#### 交叉熵损失

$$\mathcal{L}_{\text{CE}} = -\frac{1}{B} \sum_{b=1}^{B} \sum_{t=1}^{T} \sum_{v=1}^{V} y_{b,t,v} \log(\hat{y}_{b,t,v})$$

其中：
- $B$：批次大小
- $T$：序列长度
- $V$：词汇表大小
- $y_{b,t,v}$：真实标签（one-hot）
- $\hat{y}_{b,t,v}$：模型预测概率

#### 平滑损失曲线

使用指数移动平均（EMA）平滑损失：

$$\mathcal{L}_{\text{EMA}}^{(t)} = \beta \cdot \mathcal{L}_{\text{EMA}}^{(t-1)} + (1-\beta) \cdot \mathcal{L}^{(t)}$$

推荐$\beta = 0.9$或$0.99$。

#### 关键观察点

1. **初始下降速度**：反映优化器效率
2. **平台期位置**：反映模型容量
3. **验证-训练差距**：反映过拟合程度
4. **震荡幅度**：反映训练稳定性

---

### 1.3 收敛速度

#### 定义与计算

**达到目标性能所需步数**：

$$\text{Steps}_{\text{target}} = \min\{ t : \text{PPL}_t \leq \text{PPL}_{\text{target}} \}$$

**收敛速率（指数拟合）**：

$$\text{PPL}(t) = \text{PPL}_{\infty} + (\text{PPL}_0 - \text{PPL}_{\infty}) \cdot e^{-\lambda t}$$

其中$\lambda$为收敛速率参数。

**半衰期**：

$$t_{1/2} = \frac{\ln 2}{\lambda}$$

#### 评估指标

| 指标 | 公式 | 说明 |
|------|------|------|
| 达到目标PPL步数 | $T_{\text{target}}$ | 越小越好 |
| 收敛时间 | $T_{\epsilon} = \min\{t: |\text{PPL}_t - \text{PPL}_{t-1}| < \epsilon\}$ | 稳定阈值 |
| 收敛效率 | $\eta = \frac{\text{PPL}_{\text{final}} - \text{PPL}_{\text{init}}}{T_{\text{converge}}}$ | 单位时间改进 |

---

### 1.4 最终性能

#### 下游任务评估

**GLUE Benchmark得分**：

$$\text{GLUE-Score} = \frac{1}{|\mathcal{T}|} \sum_{\tau \in \mathcal{T}} \text{Score}_\tau$$

其中$\mathcal{T}$为GLUE任务集合。

**零样本/少样本性能**：

$$\text{Zero-shot Acc} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[\hat{y}_i = y_i]$$

#### 长上下文评估

**长程依赖测试（Long Range Arena）**：

$$\text{LRA-Score} = \frac{1}{6} \sum_{k=1}^{6} \text{Acc}_k$$

包含：ListOps、Text、Retrieval、Image、Pathfinder、Path-X

---

## 2. 效率指标

### 2.1 训练吞吐量（Tokens/sec）

#### 定义

$$\text{Throughput} = \frac{B \times T \times N_{\text{steps}}}{\Delta t} \quad [\text{tokens/second}]$$

其中：
- $B$：批次大小
- $T$：序列长度
- $N_{\text{steps}}$：步数
- $\Delta t$：总时间（秒）

#### 硬件利用率

**GPU利用率**：
$$\text{GPU Util} = \frac{\text{实际计算时间}}{\text{总时间}} \times 100\%$$

**FLOPs利用率**：
$$\text{FLOPs Util} = \frac{\text{实际FLOPs}}{\text{理论峰值FLOPs}} \times 100\%$$

#### 扩展效率

**数据并行扩展效率**：
$$\eta_{\text{data}} = \frac{\text{Throughput}_N}{N \times \text{Throughput}_1}$$

**模型并行扩展效率**：
$$\eta_{\text{model}} = \frac{T_{\text{single}}}{N \times T_{\text{parallel}}}$$

---

### 2.2 显存占用（Peak Memory）

#### 峰值显存计算

$$\text{Mem}_{\text{peak}} = \max_t \text{Mem}(t)$$

#### 显存分解

$$\text{Mem}_{\text{total}} = \text{Mem}_{\text{params}} + \text{Mem}_{\text{grads}} + \text{Mem}_{\text{activations}} + \text{Mem}_{\text{optimizer}}$$

各组成部分：

| 组件 | 计算公式 | 说明 |
|------|----------|------|
| 参数显存 | $4 \times P$ | FP32参数，$P$为参数量 |
| 梯度显存 | $4 \times P$ | FP32梯度 |
| 激活显存 | $8 \times B \times T \times L \times H \times D$ | 与序列长度相关 |
| 优化器显存 | $8 \times P$ (Adam) | 一阶和二阶动量 |

#### 显存效率

$$\text{Mem Efficiency} = \frac{\text{实际使用显存}}{\text{GPU总显存}} \times 100\%$$

---

### 2.3 训练时间

#### 总训练时间

$$T_{\text{total}} = T_{\text{compute}} + T_{\text{comm}} + T_{\text{io}}$$

其中：
- $T_{\text{compute}}$：实际计算时间
- $T_{\text{comm}}$：通信时间（多卡）
- $T_{\text{io}}$：数据加载时间

#### 时间效率

**每步平均时间**：
$$\bar{t}_{\text{step}} = \frac{T_{\text{total}}}{N_{\text{steps}}}$$

**达到目标时间**：
$$T_{\text{target}} = \min\{ t : \text{PPL}(t) \leq \text{PPL}_{\text{target}} \}$$

---

### 2.4 FLOPs（浮点运算次数）

#### 注意力机制FLOPs

**标准自注意力**：
$$\text{FLOPs}_{\text{attn}} = 2 \times B \times L \times H \times (T^2 \times D + T \times D^2)$$

**正交随机注意力**（假设使用$R$个随机投影）：
$$\text{FLOPs}_{\text{ortho-rand}} = 2 \times B \times L \times H \times (T \times R \times D + R \times D^2)$$

#### 总FLOPs

$$\text{FLOPs}_{\text{total}} = \text{FLOPs}_{\text{attn}} + \text{FLOPs}_{\text{ffn}} + \text{FLOPs}_{\text{emb}}$$

#### FLOPs效率

$$\text{FLOPs Efficiency} = \frac{\text{实际吞吐量}}{\text{理论吞吐量}} = \frac{\text{Tokens/sec}}{\text{Peak FLOPs} / \text{FLOPs per token}}$$

---

## 3. 稳定性指标

### 3.1 梯度范数

#### 全局梯度范数

$$\| G \|_2 = \sqrt{\sum_{p \in \theta} \| \nabla_p \mathcal{L} \|_2^2}$$

其中$\theta$为所有可训练参数。

#### 逐层梯度范数

$$\| G^{(l)} \|_2 = \sqrt{\sum_{p \in \theta^{(l)}} \| \nabla_p \mathcal{L} \|_2^2}$$

#### 梯度裁剪比率

$$\text{Clip Ratio} = \frac{\text{被裁剪的梯度数量}}{\text{总梯度数量}}$$

#### 梯度噪声尺度

$$\text{GNS} = \frac{\text{tr}(\Sigma)}{\| \mathbb{E}[G] \|_2^2}$$

其中$\Sigma$为梯度协方差矩阵。

---

### 3.2 权重范数变化

#### 权重范数

**L2范数**：
$$\| W^{(l)} \|_2 = \sqrt{\sum_{i,j} (W_{ij}^{(l)})^2}$$

**谱范数**：
$$\| W^{(l)} \|_{\text{spec}} = \sigma_{\max}(W^{(l)})$$

#### 权重变化率

$$\Delta W^{(l)}(t) = \frac{\| W^{(l)}(t) - W^{(l)}(t-1) \|_2}{\| W^{(l)}(t-1) \|_2}$$

#### 有效秩

$$\text{eff-rank}(W) = \exp\left( -\sum_i p_i \log p_i \right)$$

其中$p_i = \frac{\sigma_i^2}{\sum_j \sigma_j^2}$，$\sigma_i$为奇异值。

---

### 3.3 注意力熵

#### 注意力分布熵

对于注意力矩阵$A \in \mathbb{R}^{T \times T}$：

$$H(A_i) = -\sum_{j=1}^{T} A_{ij} \log A_{ij}$$

平均注意力熵：
$$\bar{H} = \frac{1}{T} \sum_{i=1}^{T} H(A_i)$$

#### 最大注意力权重

$$\text{Max-Attn}_i = \max_j A_{ij}$$

#### 注意力集中度

$$\text{Concentration} = \frac{1}{T} \sum_{i=1}^{T} \text{Max-Attn}_i$$

#### 注意力稀疏度

$$\text{Sparsity} = \frac{\|\{ A_{ij} > \epsilon \}\|}{T^2}$$

---

### 3.4 训练稳定性

#### 损失震荡指标

**变异系数**：
$$\text{CV} = \frac{\sigma_{\mathcal{L}}}{\mu_{\mathcal{L}}}$$

**损失范围**：
$$\text{Range} = \max_t \mathcal{L}(t) - \min_t \mathcal{L}(t)$$

#### 梯度爆炸/消失检测

$$\text{Explosion} = \mathbb{1}[\|G\|_2 > \tau_{\text{upper}}]$$
$$\text{Vanishing} = \mathbb{1}[\|G\|_2 < \tau_{\text{lower}}]$$

#### 训练崩溃检测

$$\text{Collapse} = \mathbb{1}[\mathcal{L}(t) > 10 \times \mathcal{L}(0) \text{ or } \text{NaN}]$$

---

## 4. 统计显著性检验

### 4.1 多次运行的方差分析

#### 均值与标准差

对于$N$次独立运行：

$$\bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i$$

$$s = \sqrt{\frac{1}{N-1} \sum_{i=1}^{N} (x_i - \bar{x})^2}$$

#### 变异系数

$$\text{CV} = \frac{s}{\bar{x}} \times 100\%$$

### 4.2 置信区间

#### 95%置信区间

$$CI_{95} = \bar{x} \pm t_{0.025, N-1} \times \frac{s}{\sqrt{N}}$$

其中$t_{0.025, N-1}$为t分布的临界值。

#### 自助法置信区间

1. 从原始样本中有放回地抽取$B$个bootstrap样本
2. 计算每个bootstrap样本的统计量$\theta^*_b$
3. 取$\theta^*_{(\alpha/2)}$和$\theta^*_{(1-\alpha/2)}$作为置信区间

### 4.3 显著性检验

#### 配对t检验

比较两种方法（正交随机注意力 vs 标准注意力）：

$$t = \frac{\bar{d}}{s_d / \sqrt{N}}$$

其中$d_i = x_i^{\text{ortho}} - x_i^{\text{baseline}}$。

#### Wilcoxon符号秩检验（非参数）

适用于不满足正态假设的情况。

#### 效应量

**Cohen's d**：
$$d = \frac{\bar{x}_1 - \bar{x}_2}{s_{\text{pooled}}}$$

| d值 | 效应大小 |
|-----|----------|
| 0.2 | 小 |
| 0.5 | 中 |
| 0.8 | 大 |

#### 多重比较校正

**Bonferroni校正**：
$$\alpha_{\text{adjusted}} = \frac{\alpha}{m}$$

**FDR控制（Benjamini-Hochberg）**：

对p值排序后，找到最大的$k$使得$p_{(k)} \leq \frac{k}{m} \alpha$。

---

## 5. 可视化方案

### 5.1 损失曲线

#### 单图展示

```python
# 伪代码
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(steps, train_loss, label='Train Loss', alpha=0.7)
ax.plot(steps, val_loss, label='Val Loss', alpha=0.7)
ax.plot(steps, train_loss_ema, label='Train Loss (EMA)', linewidth=2)
ax.set_xlabel('Steps')
ax.set_ylabel('Loss')
ax.set_yscale('log')
ax.legend()
ax.grid(True, alpha=0.3)
```

#### 多方法对比

- 使用不同颜色区分方法
- 添加阴影表示置信区间
- 标注关键检查点

### 5.2 注意力热力图

#### 注意力权重可视化

```python
# 伪代码
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
for i, ax in enumerate(axes.flat):
    attn = attention_weights[i]  # [seq_len, seq_len]
    sns.heatmap(attn, ax=ax, cmap='viridis', cbar=True)
    ax.set_title(f'Head {i+1}')
```

#### 注意力模式分析

- 对角线注意力（局部模式）
- 垂直条纹（全局模式）
- 稀疏模式（正交随机特征）

### 5.3 梯度范数曲线

#### 全局梯度范数

```python
# 伪代码
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
ax1.plot(steps, grad_norm_global, label='Global Grad Norm')
ax1.axhline(y=clip_threshold, color='r', linestyle='--', label='Clip Threshold')
ax1.set_ylabel('Gradient Norm')
ax1.set_yscale('log')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 逐层梯度范数
for layer_id in range(num_layers):
    ax2.plot(steps, grad_norm_layers[layer_id], label=f'Layer {layer_id}')
ax2.set_xlabel('Steps')
ax2.set_ylabel('Layer-wise Grad Norm')
ax2.set_yscale('log')
ax2.legend(ncol=4)
ax2.grid(True, alpha=0.3)
```

### 5.4 效率对比图

#### 雷达图

```python
# 伪代码
categories = ['Throughput', 'Memory Efficiency', 'Convergence Speed', 
              'Final PPL', 'Stability']
values_ortho = [0.9, 0.85, 0.95, 0.88, 0.92]
values_baseline = [0.7, 0.6, 0.8, 0.85, 0.75]

fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(111, projection='polar')
angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
values_ortho += values_ortho[:1]
values_baseline += values_baseline[:1]
angles += angles[:1]

ax.plot(angles, values_ortho, 'o-', linewidth=2, label='Ortho-Rand')
ax.plot(angles, values_baseline, 'o-', linewidth=2, label='Baseline')
ax.fill(angles, values_ortho, alpha=0.25)
ax.fill(angles, values_baseline, alpha=0.25)
ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories)
ax.legend()
```

### 5.5 综合仪表板

```python
# 伪代码
fig = plt.figure(figsize=(16, 12))
gs = fig.add_gridspec(3, 3)

# 损失曲线
ax1 = fig.add_subplot(gs[0, :2])
# 绘制损失曲线...

# 困惑度
ax2 = fig.add_subplot(gs[0, 2])
# 绘制PPL对比...

# 注意力热力图
ax3 = fig.add_subplot(gs[1, 0])
# 绘制热力图...

# 梯度范数
ax4 = fig.add_subplot(gs[1, 1:])
# 绘制梯度曲线...

# 效率指标
ax5 = fig.add_subplot(gs[2, :])
# 绘制效率对比...
```

---

## 6. 指标对比表

### 6.1 主要评估指标对比

| 指标 | 公式 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|----------|
| **Perplexity** | $\exp(-\frac{1}{N}\sum \log P)$ | 直观、标准化 | 对短序列敏感 | 语言模型通用评估 |
| **Loss** | $-\sum y \log \hat{y}$ | 训练直接优化 | 数值范围大 | 训练监控 |
| **Convergence Speed** | $T_{\text{target}}$ | 直接反映效率 | 依赖目标设定 | 方法对比 |
| **Final Performance** | 下游任务得分 | 实用性强 | 计算成本高 | 最终评估 |

### 6.2 效率指标对比

| 指标 | 公式 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|----------|
| **Throughput** | $\frac{B\times T}{\Delta t}$ | 直接可测 | 硬件依赖 | 生产部署 |
| **Peak Memory** | $\max_t \text{Mem}(t)$ | 资源规划 | 峰值难捕获 | 硬件选择 |
| **Training Time** | $T_{\text{total}}$ | 最直观 | 环境依赖 | 项目规划 |
| **FLOPs** | 计算操作数 | 硬件无关 | 理论值 | 算法分析 |

### 6.3 稳定性指标对比

| 指标 | 公式 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|----------|
| **Gradient Norm** | $\|G\|_2$ | 检测爆炸/消失 | 阈值难定 | 训练调试 |
| **Weight Change** | $\Delta W / W$ | 反映学习动态 | 需要基线 | 收敛分析 |
| **Attention Entropy** | $-\sum A \log A$ | 分析注意力模式 | 解释性有限 | 注意力研究 |
| **Loss Variance** | $\sigma^2_{\mathcal{L}}$ | 检测不稳定 | 需要平滑 | 训练监控 |

---

## 7. 实施建议

### 7.1 实验设计

#### 最小实验配置

| 组件 | 建议配置 |
|------|----------|
| 随机种子 | 至少3个（42, 123, 2024） |
| 运行次数 | 每个配置至少3次 |
| 检查点频率 | 每1000步 |
| 日志频率 | 每100步 |
| 早停耐心 | 10个验证周期 |

#### 超参数网格

```yaml
learning_rate: [1e-4, 3e-4, 1e-3]
batch_size: [32, 64, 128]
warmup_steps: [1000, 2000, 4000]
max_seq_length: [512, 1024, 2048]
```

### 7.2 日志记录

#### 推荐日志格式

```json
{
  "step": 1000,
  "train_loss": 2.345,
  "val_loss": 2.567,
  "train_ppl": 10.43,
  "val_ppl": 13.02,
  "grad_norm": 1.23,
  "learning_rate": 0.0003,
  "throughput": 12500.5,
  "peak_memory_mb": 8192,
  "attention_entropy": 3.45,
  "timestamp": "2024-01-15T10:30:00Z"
}
```

### 7.3 报告模板

#### 实验报告结构

1. **实验配置**
   - 模型架构
   - 训练超参数
   - 硬件环境

2. **主要结果**
   - 最终PPL（均值±标准差）
   - 收敛时间
   - 下游任务性能

3. **效率分析**
   - 吞吐量对比
   - 显存占用
   - 训练时间

4. **稳定性分析**
   - 损失曲线
   - 梯度范数
   - 多次运行方差

5. **统计检验**
   - t检验结果
   - 效应量
   - 置信区间

### 7.4 工具推荐

| 用途 | 工具 | 说明 |
|------|------|------|
| 实验跟踪 | Weights & Biases, TensorBoard | 实时可视化 |
| 统计分析 | SciPy, Statsmodels | 假设检验 |
| 可视化 | Matplotlib, Seaborn, Plotly | 静态/交互式 |
| 性能分析 | PyTorch Profiler, Nsight | 性能瓶颈 |

---

## 附录

### A. 指标计算公式汇总

#### A.1 困惑度相关

```
PPL = exp(-mean(log_probs))
BPB = -sum(log2_probs) / total_chars
Bits Per Token = -mean(log2_probs)
```

#### A.2 效率相关

```
Throughput = tokens_processed / time_seconds
FLOPs = 2 * params * tokens * 3 (forward+backward)
Memory = params * 4 + grads * 4 + optimizer * 8 + activations
```

#### A.3 稳定性相关

```
Grad Norm = sqrt(sum(grad^2))
Weight Change = norm(W_t - W_{t-1}) / norm(W_{t-1})
Attention Entropy = -sum(A * log(A + eps))
```

### B. 参考阈值

| 指标 | 健康范围 | 警告阈值 | 危险阈值 |
|------|----------|----------|----------|
| Grad Norm | 0.1 - 10 | > 100 | > 1000 |
| Loss Spike | < 2x | 2-5x | > 5x |
| PPL Increase | < 5% | 5-20% | > 20% |
| Memory Usage | < 80% | 80-95% | > 95% |

---

*文档版本: 1.0*
*最后更新: 2024年*
*作者: ML评估专家*
