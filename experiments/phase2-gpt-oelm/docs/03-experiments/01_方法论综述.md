# 正交随机注意力方法设计综合审查报告

## 摘要

本报告综合审查了正交随机注意力（Orthogonal Random Attention, ORA）机制的方法设计，整合了前4个Agent的设计成果：正交初始化算法（Agent 10）、表达能力分析（Agent 11），以及第一阶段综合理论分析（Agent 8）。通过系统性审查，验证了架构设计的完整性和一致性，识别了潜在的设计风险，并提出了改进建议和实验验证需求。

**核心发现**：
1. 正交随机注意力的架构设计具有坚实的理论基础，整合了Synthesizer、Reservoir Computing、正交神经网络和ELM四大理论支柱
2. QR分解被选为正交初始化的推荐方法，平衡了计算效率和数值稳定性
3. 表达能力分析证明ORA在概率意义下保持通用逼近能力，损失以$O(1/\sqrt{d_k})$衰减
4. 设计决策整体一致，但存在多头独立性验证、长序列适应性等需要实验验证的假设

---

## 1. 架构设计综合

### 1.1 设计组件整合

基于已有设计文档，正交随机注意力的完整架构包含以下核心组件：

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        正交随机注意力完整架构                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        输入层 (Input Layer)                          │   │
│  │              X ∈ ℝ^(batch × seq_len × d_model)                      │   │
│  └────────────────────────┬────────────────────────────────────────────┘   │
│                           │                                                 │
│                           ▼                                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                    正交随机注意力层 (ORA Layer)                       │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                 │   │
│  │  │   W_Q^orth  │  │   W_K^orth  │  │    W_V      │                 │   │
│  │  │   (冻结)     │  │   (冻结)     │  │  (可训练)    │                 │   │
│  │  │  QR分解初始化│  │  QR分解初始化│  │  标准初始化  │                 │   │
│  │  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘                 │   │
│  │         │                │                │                         │   │
│  │         ▼                ▼                ▼                         │   │
│  │        Q = XW_Q        K = XW_K        V = XW_V                     │   │
│  │         │                │                │                         │   │
│  │         └────────────────┼────────────────┘                         │   │
│  │                          ▼                                          │   │
│  │              A = softmax(QK^T / √d_k)                               │   │
│  │                          │                                          │   │
│  │                          ▼                                          │   │
│  │                   Output = AV                                       │   │
│  └──────────────────────────┬──────────────────────────────────────────┘   │
│                             │                                               │
│                             ▼                                               │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                    前馈网络层 (FFN Layer)                            │   │
│  │              FFN(x) = σ(xW_1 + b_1)W_2 + b_2                        │   │
│  │                    (全部可训练)                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                    归一化与残差连接                                   │   │
│  │         LayerNorm, Dropout, Residual Connection                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 1.2 组件规格定义

| 组件 | 类型 | 维度 | 初始化方法 | 训练状态 |
|------|------|------|-----------|----------|
| W_Q | 正交矩阵 | (d_model, d_k) | QR分解/Haar测度 | **冻结** |
| W_K | 正交矩阵 | (d_model, d_k) | QR分解/Haar测度 | **冻结** |
| W_V | 标准矩阵 | (d_model, d_v) | Xavier/Kaiming | 可训练 |
| W_O | 标准矩阵 | (d_v, d_model) | Xavier/Kaiming | 可训练 |
| W_1 (FFN) | 标准矩阵 | (d_model, d_ff) | Xavier/Kaiming | 可训练 |
| W_2 (FFN) | 标准矩阵 | (d_ff, d_model) | Xavier/Kaiming | 可训练 |

### 1.3 设计一致性验证

#### 一致性检查清单

| 检查项 | 状态 | 说明 |
|--------|------|------|
| 正交性约束与冻结策略一致 | ✅ 通过 | 正交矩阵冻结确保约束全程保持 |
| 初始化方法与理论依据一致 | ✅ 通过 | QR分解符合Haar测度采样理论 |
| 可训练参数与表达能力补偿一致 | ✅ 通过 | V+FFN可训练补偿QK冻结 |
| 多头设计与多样性理论一致 | ⚠️ 待验证 | 假设独立采样产生多样性 |
| 数值稳定性设计与正交理论一致 | ✅ 通过 | 条件数=1保证数值稳定 |

#### 设计一致性分析

**正交初始化 ↔ 冻结策略**：
- 正交矩阵 $W^TW = I$ 是一个约束条件
- 如果允许训练，需要复杂的正交参数化或投影
- 冻结策略简化了实现，同时保持理论保证
- **一致性评级**：✅ 强一致

**随机QK ↔ 可训练V/FFN**：
- 基于ELM理论：随机特征映射 + 可训练输出层
- 基于Reservoir理论：固定随机内部 + 可训练读出
- **一致性评级**：✅ 强一致

**表达能力 ↔ 效率权衡**：
- 理论证明表达能力损失以 $O(1/\sqrt{d_k})$ 衰减
- 参数节省50%（注意力部分）
- **一致性评级**：✅ 可接受权衡

---

## 2. 设计决策分析

### 2.1 关键设计决策汇总

| 决策编号 | 决策内容 | 决策Agent | 理论依据 |
|----------|----------|-----------|----------|
| D1 | 使用正交随机矩阵而非高斯随机 | Agent 10 | 正交神经网络理论 |
| D2 | QR分解作为推荐初始化方法 | Agent 10 | 计算效率vs数值稳定性权衡 |
| D3 | 冻结QK投影，仅训练V/FFN | Agent 8, 11 | ELM + Reservoir理论 |
| D4 | 保持多头独立随机初始化 | Agent 11 | 多样性理论 |
| D5 | 序列长度无关的投影设计 | Agent 8 | Synthesizer改进 |

### 2.2 设计决策详细分析

#### 决策D1：正交随机 vs 高斯随机

**决策内容**：使用严格正交随机矩阵而非普通高斯随机矩阵初始化QK投影

**优点**：
| 优点 | 说明 |
|------|------|
| 范数保持性 | $\|Wx\|_2 = \|x\|_2$，保持向量长度 |
| 条件数最优 | $\kappa(W) = 1$，数值稳定性最佳 |
| 梯度稳定性 | 反向传播时梯度范数保持 |
| 内积保持 | $\langle xW, yW \rangle = \langle x, y \rangle$ |

**缺点**：
| 缺点 | 说明 |
|------|------|
| 初始化计算成本 | QR分解需要 $O(d_{model} d_k^2)$ |
| 实现复杂度 | 需要自定义初始化函数 |
| 多样性限制 | 正交约束限制了随机空间 |

**理论依据**：
- 正交神经网络研究表明正交性显著提升训练稳定性
- Johnson-Lindenstrauss引理：正交投影保持距离结构

**决策评级**：✅ **推荐** - 收益大于成本

---

#### 决策D2：QR分解作为推荐方法

**决策内容**：在多种正交初始化方法中选择QR分解作为推荐方法

**方法对比**：

| 方法 | 时间复杂度 | 数值稳定性 | 正交性误差 | 实现复杂度 |
|------|-----------|-----------|-----------|-----------|
| QR分解 | $O(mn^2)$ | ★★★★☆ | $10^{-14}$ | 低 |
| SVD分解 | $O(m^2n)$ | ★★★★★ | $10^{-16}$ | 低 |
| Householder | $O(mn^2)$ | ★★★★★ | $10^{-15}$ | 高 |
| Cayley变换 | $O(m^3)$ | ★★★☆☆ | $10^{-13}$ | 中 |

**选择QR分解的理由**：
1. PyTorch原生支持 `torch.linalg.qr()`
2. 计算效率最高（当 $m \gg n$ 时）
3. 数值稳定性足够（误差 $10^{-14}$ 量级）
4. 实现最简单

**替代方案**：
- **高精度场景**：使用SVD分解
- **可学习正交**：使用Cayley变换参数化

**决策评级**：✅ **推荐** - 最佳工程权衡

---

#### 决策D3：冻结QK，训练V/FFN

**决策内容**：冻结正交随机QK投影，仅训练V投影和FFN

**优点**：
| 优点 | 量化指标 |
|------|----------|
| 参数节省 | 50%注意力参数无需训练 |
| 训练加速 | 减少约30%梯度计算 |
| 内存节省 | 无需存储QK的梯度 |
| 优化简化 | 部分凸优化问题 |
| 稳定性提升 | 避免正交约束优化 |

**缺点**：
| 缺点 | 量化指标 |
|------|----------|
| 表达能力损失 | $O(1/\sqrt{d_k})$ |
| 任务适配性 | 某些任务可能需要可学习QK |
| 收敛速度 | 可能略慢于全可训练 |

**理论依据**：
- ELM理论：随机隐藏层 + 解析解输出层
- Reservoir理论：固定随机内部 + 可训练读出
- Synthesizer实验：随机注意力达到接近标准Transformer的性能

**决策评级**：✅ **推荐** - 效率收益显著，损失可控

---

#### 决策D4：多头独立随机初始化

**决策内容**：每个注意力头使用独立的正交随机QK投影

**理论假设**：
- 独立采样产生不同的正交基
- 多头组合提供表达能力叠加
- 不同头关注不同特征子空间

**优点**：
- 增加模型表达能力
- 提供注意力模式多样性
- 符合Transformer多头设计哲学

**风险**：
- 独立性假设需要验证
- 可能存在冗余头
- 最优头数需要调参

**决策评级**：⚠️ **条件推荐** - 需要实验验证独立性假设

---

#### 决策D5：序列长度无关设计

**决策内容**：通过投影矩阵计算注意力，而非直接学习注意力矩阵

**与Synthesizer对比**：

| 特性 | Synthesizer Random | 正交随机注意力 |
|------|-------------------|---------------|
| 注意力来源 | 直接学习 $R \in \mathbb{R}^{N \times N}$ | 投影计算 $QK^T$ |
| 序列长度依赖 | **依赖**（$N^2$参数） | **无关** |
| 变长序列支持 | 不支持 | 支持 |
| 参数效率 | 低 | 高 |

**优点**：
- 天然支持变长序列
- 参数数量与长度无关
- 更好的泛化能力

**决策评级**：✅ **推荐** - 显著优势

---

## 3. 潜在问题识别与缓解策略

### 3.1 设计风险矩阵

| 风险编号 | 风险描述 | 可能性 | 影响 | 风险等级 |
|----------|----------|--------|------|----------|
| R1 | 表达能力不足 | 中 | 高 | 🔴 高 |
| R2 | 任务适配性问题 | 中 | 高 | 🔴 高 |
| R3 | 多头独立性假设不成立 | 中 | 中 | 🟡 中 |
| R4 | 长序列数值稳定性 | 低 | 中 | 🟡 中 |
| R5 | 收敛速度变慢 | 中 | 低 | 🟢 低 |
| R6 | 正交初始化开销 | 低 | 低 | 🟢 低 |

### 3.2 风险详细分析

#### 风险R1：表达能力不足

**风险描述**：冻结QK投影可能限制模型的表达能力，无法学习复杂的注意力模式

**触发条件**：
- 任务需要精确的注意力对齐（如指针网络）
- 需要复杂的结构化注意力模式
- 长距离依赖建模

**缓解策略**：

| 策略 | 实现方式 | 成本 |
|------|----------|------|
| 混合架构 | 底层正交随机 + 顶层可学习QK | 中等 |
| 增加头数 | 用多样性补偿单头表达能力 | 低 |
| 渐进式训练 | 先冻结，后微调部分层 | 低 |
| 大d_k补偿 | 增加注意力维度降低损失 | 中等 |

**验证方法**：
- 在GLUE基准上对比冻结vs可学习QK
- 分析注意力分布的熵值
- 测量模型有效容量

---

#### 风险R2：任务适配性问题

**风险描述**：某些任务可能需要特定的注意力模式，正交随机投影无法提供

**高风险任务**：
- 序列到序列对齐（机器翻译）
- 指针操作（复制、排序）
- 结构化预测（句法分析）

**低风险任务**：
- 分类任务
- 语义理解
- 特征提取

**缓解策略**：

| 策略 | 适用场景 |
|------|----------|
| 任务特定混合比例 | 根据任务调整冻结/可学习比例 |
| 自适应正交选择 | 从正交矩阵族中选择适合任务的矩阵 |
| 渐进解冻 | 训练后期微调QK投影 |

---

#### 风险R3：多头独立性假设

**风险描述**：假设独立采样的正交矩阵产生多样性，但可能不成立

**验证方法**：
1. 测量不同头QK矩阵的余弦相似度
2. 分析各头注意力分布的KL散度
3. 对比单头vs多头性能

**预期结果**：
- 余弦相似度 < 0.3（低相关）
- KL散度 > 1.0（显著差异）
- 多头性能 > 单头性能

**缓解策略**：
- 如果独立性不成立，考虑结构化正交矩阵
- 使用Hadamard矩阵等确定性正交基

---

#### 风险R4：长序列数值稳定性

**风险描述**：尽管正交矩阵条件数为1，但多层堆叠和长序列可能导致数值问题

**分析**：
- Softmax在长序列上可能产生极小的梯度
- 多层堆叠可能放大数值误差
- LayerNorm和残差连接可以缓解

**缓解策略**：
- 使用Pre-LN架构
- 梯度裁剪
- 考虑使用线性注意力变体

---

### 3.3 风险缓解优先级

```
优先级矩阵：
                    影响
              低        中        高
         ┌─────────┬─────────┬─────────┐
    高   │         │  R3     │  R1, R2 │
可       │         │  多头   │  表达   │
能       ├─────────┼─────────┼─────────┤
性  中   │  R5     │         │         │
         │  收敛   │         │         │
         ├─────────┼─────────┼─────────┤
    低   │  R6     │  R4     │         │
         │  初始化 │  长序列 │         │
         └─────────┴─────────┴─────────┘

优先级：R1, R2 > R3 > R4 > R5 > R6
```

---

## 4. 改进建议

### 4.1 架构改进

#### 改进I1：自适应混合架构

**建议**：设计自适应机制，根据任务和层深度动态调整冻结/可学习比例

**实现方案**：
```
层1-4:  正交随机QK (冻结)
层5-8:  混合 (部分头冻结，部分头可学习)
层9-12: 可学习QK
```

**预期收益**：
- 底层学习通用特征（冻结随机足够）
- 顶层学习任务特定模式（需要可学习）
- 平衡效率和表达能力

**实施成本**：中等

---

#### 改进I2：结构化正交矩阵

**建议**：探索结构化正交矩阵（如Hadamard、DCT）替代随机正交矩阵

**候选矩阵**：
| 矩阵类型 | 计算效率 | 多样性 | 适用场景 |
|----------|----------|--------|----------|
| Haar随机 | 中 | 高 | 通用 |
| Hadamard | 高 | 低 | 快速推理 |
| DCT | 高 | 中 | 信号处理任务 |
| 小波基 | 中 | 中 | 多尺度任务 |

**预期收益**：
- Hadamard：无需初始化计算，O(n log n)快速乘法
- DCT：适合处理序列数据的频率特征

**实施成本**：低

---

#### 改进I3：动态维度调整

**建议**：根据任务复杂度动态调整$d_k$维度

**策略**：
```
简单任务: d_k = 32
中等任务: d_k = 64
复杂任务: d_k = 128
```

**理论依据**：
- 表达能力损失 $O(1/\sqrt{d_k})$
- 更大的$d_k$补偿冻结QK的损失

**实施成本**：低

---

### 4.2 训练改进

#### 改进I4：渐进式解冻策略

**建议**：训练过程中逐步解冻QK投影

**时间表**：
| 阶段 | 轮次 | QK状态 | 学习率 |
|------|------|--------|--------|
| 1 | 0-30% | 冻结 | 正常 |
| 2 | 30-60% | 部分解冻 | 降低50% |
| 3 | 60-100% | 可选全解冻 | 降低80% |

**预期收益**：
- 早期快速收敛（冻结简化优化）
- 后期精细调整（解冻提升性能）

**实施成本**：低

---

#### 改进I5：正交正则化替代冻结

**建议**：考虑使用正交正则化而非完全冻结

**损失函数**：
$$\mathcal{L}_{total} = \mathcal{L}_{task} + \lambda \|W_Q^T W_Q - I\|_F^2 + \lambda \|W_K^T W_K - I\|_F^2$$

**权衡**：
| 方案 | 优点 | 缺点 |
|------|------|------|
| 冻结 | 简单高效，理论保证 | 完全固定，无适应性 |
| 正则化 | 保持正交性同时可学习 | 增加超参数，计算成本 |

**实施成本**：中等

---

### 4.3 改进建议优先级

| 改进编号 | 改进内容 | 收益 | 成本 | 优先级 |
|----------|----------|------|------|--------|
| I1 | 自适应混合架构 | 高 | 中 | P1 |
| I2 | 结构化正交矩阵 | 中 | 低 | P2 |
| I3 | 动态维度调整 | 中 | 低 | P2 |
| I4 | 渐进式解冻 | 中 | 低 | P2 |
| I5 | 正交正则化 | 中 | 中 | P3 |

---

## 5. 实验验证需求

### 5.1 关键假设验证

| 假设编号 | 假设陈述 | 验证方法 | 成功标准 |
|----------|----------|----------|----------|
| H1 | 正交随机注意力具有通用逼近能力 | 函数逼近实验 | MSE随规模趋于0 |
| H2 | 正交性提升训练稳定性 | 训练曲线对比 | 梯度范数变化<30% |
| H3 | 冻结QK不显著降低表达能力 | GLUE基准测试 | 性能差距<5% |
| H4 | 多头独立采样产生多样性 | 余弦相似度分析 | 相似度<0.3 |
| H5 | 支持变长序列 | 长度泛化实验 | ±50%长度内性能稳定 |

### 5.2 实验设计

#### 实验E1：通用逼近能力验证

**目的**：验证正交随机注意力的通用逼近能力

**设计**：
- 合成函数逼近任务
- 对比标准Transformer vs 正交随机Transformer
- 变化模型规模（层数、维度）

**指标**：
- MSE vs 模型规模
- 收敛速度
- 最终逼近精度

**预期结果**：
- 随着规模增大，MSE趋于0
- 正交随机与标准差距<10%

---

#### 实验E2：训练稳定性对比

**目的**：验证正交性对训练稳定性的提升

**设计**：
- 在WikiText-2上训练
- 对比三种初始化：
  1. 标准Xavier初始化
  2. 高斯随机初始化
  3. 正交随机初始化

**指标**：
- 损失曲线平滑度
- 梯度范数变化
- 训练时间

**预期结果**：
- 正交初始化训练最稳定
- 梯度范数变化最小

---

#### 实验E3：表达能力对比

**目的**：量化冻结QK的表达能力损失

**设计**：
- 在GLUE基准上测试
- 对比三种配置：
  1. 标准Transformer（全可训练）
  2. 正交随机QK（冻结）
  3. 高斯随机QK（冻结）

**指标**：
- 各任务准确率
- 平均性能差距

**预期结果**：
- 正交随机 vs 标准：差距<5%
- 正交随机 vs 高斯随机：正交更好

---

#### 实验E4：多头多样性验证

**目的**：验证多头独立采样的多样性假设

**设计**：
- 训练完成后分析各头
- 测量QK矩阵相似度
- 分析注意力分布差异

**指标**：
- QK矩阵余弦相似度
- 注意力分布KL散度
- 单头vs多头性能

**预期结果**：
- 余弦相似度<0.3
- KL散度>1.0
- 多头性能>单头性能

---

#### 实验E5：变长序列泛化

**目的**：验证序列长度无关设计的优势

**设计**：
- 在固定长度上训练
- 在不同长度上测试
- 对比Synthesizer（长度绑定）

**指标**：
- 困惑度随长度变化
- 性能下降幅度

**预期结果**：
- ±50%长度内性能稳定
- 优于Synthesizer的长度泛化

---

### 5.3 实验优先级

```
实验优先级：

P0 (必需):
├── E3: 表达能力对比 (验证核心假设)
└── E2: 训练稳定性 (验证正交性优势)

P1 (重要):
├── E1: 通用逼近能力 (理论验证)
└── E4: 多头多样性 (设计验证)

P2 (补充):
└── E5: 变长序列泛化 (特性验证)
```

---

## 6. 完整架构图

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    正交随机Transformer完整架构                                    │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│   Input X ∈ ℝ^(B×N×d)                                                          │
│      │                                                                          │
│      ▼                                                                          │
│   ┌─────────────────────────────────────────────────────────────────────────┐  │
│   │                         嵌入层 (Embeddings)                              │  │
│   │              Token Embedding + Positional Encoding                        │  │
│   └─────────────────────────────────────────────────────────────────────────┘  │
│      │                                                                          │
│      ▼                                                                          │
│   ┌─────────────────────────────────────────────────────────────────────────┐  │
│   │                      编码器层 × L (重复L次)                               │  │
│   │  ┌─────────────────────────────────────────────────────────────────┐   │  │
│   │  │                    子层1: 多头正交随机注意力                        │   │  │
│   │  │  ┌───────────────────────────────────────────────────────────┐  │   │  │
│   │  │  │  头1: Q=XW_Q^orth, K=XW_K^orth, V=XW_V (W_Q, W_K冻结)      │  │   │  │
│   │  │  │  头2: Q=XW_Q^orth, K=XW_K^orth, V=XW_V (独立初始化)        │  │   │  │
│   │  │  │  ...                                                        │  │   │  │
│   │  │  │  头h: Q=XW_Q^orth, K=XW_K^orth, V=XW_V                     │  │   │  │
│   │  │  └───────────────────────────────────────────────────────────┘  │   │  │
│   │  │  Concat(head_1, ..., head_h) @ W_O                              │   │  │
│   │  └─────────────────────────────────────────────────────────────────┘   │  │
│   │      │                                                                      │
│   │      ├──→ [残差连接] ──→ [LayerNorm] ───────────────────────────────────────│
│   │      │                                                                      │
│   │      ▼                                                                      │
│   │  ┌─────────────────────────────────────────────────────────────────┐   │  │
│   │  │                    子层2: 前馈网络 (FFN)                          │   │  │
│   │  │              FFN(x) = GELU(xW_1 + b_1)W_2 + b_2                 │   │  │
│   │  └─────────────────────────────────────────────────────────────────┘   │  │
│   │      │                                                                      │
│   │      └──→ [残差连接] ──→ [LayerNorm] ───────────────────────────────────────│
│   └─────────────────────────────────────────────────────────────────────────┘  │
│      │                                                                          │
│      ▼                                                                          │
│   ┌─────────────────────────────────────────────────────────────────────────┐  │
│   │                        输出层 (Output Layer)                             │  │
│   │                    Linear(d_model, vocab_size)                          │  │
│   └─────────────────────────────────────────────────────────────────────────┘  │
│      │                                                                          │
│      ▼                                                                          │
│   Output Logits ∈ ℝ^(B×N×vocab_size)                                           │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 7. 总结

### 7.1 设计审查结论

| 审查项 | 结论 |
|--------|------|
| 架构完整性 | ✅ 完整 - 所有必要组件已定义 |
| 设计一致性 | ✅ 一致 - 各组件间逻辑自洽 |
| 理论基础 | ✅ 坚实 - 四大理论支柱支撑 |
| 实现可行性 | ✅ 可行 - PyTorch可实现 |
| 风险可控性 | ⚠️ 基本可控 - 需要实验验证 |

### 7.2 关键发现

1. **正交初始化选择QR分解是合理的** - 平衡了计算效率和数值稳定性
2. **冻结QK策略有理论支撑** - ELM和Reservoir理论支持固定随机+可训练输出范式
3. **表达能力损失可控** - 理论证明损失以 $O(1/\sqrt{d_k})$ 衰减
4. **多头独立性需要验证** - 这是设计中的关键假设
5. **混合架构值得探索** - 自适应冻结/可学习比例可能是最优方案

### 7.3 下一步行动建议

| 优先级 | 行动项 | 负责人 |
|--------|--------|--------|
| P0 | 实现正交随机注意力原型 | 开发团队 |
| P0 | 执行实验E2（训练稳定性） | 实验团队 |
| P0 | 执行实验E3（表达能力对比） | 实验团队 |
| P1 | 执行实验E1（通用逼近） | 理论团队 |
| P1 | 执行实验E4（多头多样性） | 实验团队 |
| P2 | 探索混合架构设计 | 架构团队 |
| P2 | 研究结构化正交矩阵 | 理论团队 |

---

## 附录A：符号表

| 符号 | 含义 | 典型值 |
|------|------|--------|
| $d_{model}$ | 模型隐藏维度 | 768 |
| $d_k$ | 注意力键/查询维度 | 64 |
| $d_v$ | 注意力值维度 | 64 |
| $d_{ff}$ | FFN隐藏维度 | 3072 |
| $h$ | 注意力头数 | 12 |
| $L$ | 层数 | 12 |
| $N$ | 序列长度 | 可变 |
| $B$ | 批量大小 | - |

## 附录B：术语表

| 术语 | 英文 | 说明 |
|------|------|------|
| 正交随机注意力 | Orthogonal Random Attention (ORA) | 本研究提出的注意力机制 |
| QR分解 | QR Decomposition | 矩阵分解方法，$A = QR$ |
| Haar测度 | Haar Measure | 正交群上的均匀分布 |
| ELM | Extreme Learning Machine | 极限学习机 |
| ESP | Echo State Property | 回声状态性质 |

---

*报告生成日期：2025年2月*
*基于Agent 8, 10, 11的设计成果综合审查*
