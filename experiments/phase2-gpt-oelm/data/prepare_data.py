#!/usr/bin/env python3
"""
Data Preprocessing Script for Language Model Training

This script downloads and preprocesses datasets for training language models.
Supported datasets:
- TinyStories: Small dataset for quick experiments
- OpenWebText: Medium-scale dataset
- C4: Large-scale dataset (subset)

Usage:
    # Prepare TinyStories
    python prepare_data.py --dataset tinystories --output_dir data/tinystories
    
    # Prepare OpenWebText
    python prepare_data.py --dataset openwebtext --output_dir data/openwebtext
    
    # Prepare custom dataset
    python prepare_data.py --dataset custom --data_path /path/to/text/files --output_dir data/custom
"""

import os
import sys
import argparse
import json
from pathlib import Path
from typing import List, Optional
import multiprocessing as mp

import numpy as np
from tqdm import tqdm

# Try to import datasets and tiktoken
try:
    import datasets
    from datasets import load_dataset
    HAS_DATASETS = True
except ImportError:
    HAS_DATASETS = False
    print("Warning: datasets library not installed. Some features may not work.")

try:
    import tiktoken
    HAS_TIKTOKEN = True
except ImportError:
    HAS_TIKTOKEN = False
    print("Warning: tiktoken not installed. Using character-level tokenization.")


class Tokenizer:
    """Simple tokenizer wrapper."""
    
    def __init__(self, name: str = 'gpt2'):
        self.name = name
        
        if HAS_TIKTOKEN and name == 'gpt2':
            self.enc = tiktoken.get_encoding("gpt2")
            self.vocab_size = self.enc.n_vocab
        else:
            # Simple character-level fallback
            self.enc = None
            self.vocab_size = 256  # ASCII
    
    def encode(self, text: str) -> List[int]:
        """Encode text to token IDs."""
        if self.enc is not None:
            return self.enc.encode(text)
        else:
            # Character-level encoding
            return [ord(c) % 256 for c in text]
    
    def decode(self, token_ids: List[int]) -> str:
        """Decode token IDs to text."""
        if self.enc is not None:
            return self.enc.decode(token_ids)
        else:
            # Character-level decoding
            return ''.join(chr(i % 256) for i in token_ids)


def prepare_tinystories(output_dir: str, tokenizer: Tokenizer, val_ratio: float = 0.01):
    """
    Download and prepare TinyStories dataset.
    
    TinyStories is a dataset of short stories generated by GPT-4, suitable for
    training small language models.
    
    Args:
        output_dir: Output directory for processed data
        tokenizer: Tokenizer to use
        val_ratio: Ratio of data to use for validation
    """
    if not HAS_DATASETS:
        raise ImportError("datasets library is required for TinyStories")
    
    print("Downloading TinyStories dataset...")
    dataset = load_dataset("roneneldan/TinyStories", split="train")
    
    # Split into train/val
    dataset = dataset.train_test_split(test_size=val_ratio, seed=42)
    train_data = dataset['train']
    val_data = dataset['test']
    
    print(f"Train samples: {len(train_data)}")
    print(f"Val samples: {len(val_data)}")
    
    # Tokenize and save
    os.makedirs(output_dir, exist_ok=True)
    
    for split_name, split_data in [('train', train_data), ('val', val_data)]:
        print(f"\nProcessing {split_name} split...")
        
        all_tokens = []
        for example in tqdm(split_data, desc=f"Tokenizing {split_name}"):
            text = example['text']
            tokens = tokenizer.encode(text)
            all_tokens.extend(tokens)
            # Add EOS token
            all_tokens.append(tokenizer.vocab_size - 1)
        
        # Save as numpy array
        tokens_array = np.array(all_tokens, dtype=np.uint16)
        output_path = os.path.join(output_dir, f'{split_name}.bin')
        tokens_array.tofile(output_path)
        
        print(f"  Saved {len(tokens_array):,} tokens to {output_path}")
        print(f"  Size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
    
    # Save metadata
    metadata = {
        'dataset': 'tinystories',
        'vocab_size': tokenizer.vocab_size,
        'tokenizer': tokenizer.name,
        'train_tokens': len(train_data),
        'val_tokens': len(val_data)
    }
    
    with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nDataset preparation complete!")


def prepare_openwebtext(output_dir: str, tokenizer: Tokenizer, val_ratio: float = 0.01):
    """
    Download and prepare OpenWebText dataset.
    
    OpenWebText is an open-source replication of WebText (used to train GPT-2).
    
    Args:
        output_dir: Output directory for processed data
        tokenizer: Tokenizer to use
        val_ratio: Ratio of data to use for validation
    """
    if not HAS_DATASETS:
        raise ImportError("datasets library is required for OpenWebText")
    
    print("Downloading OpenWebText dataset...")
    print("Note: This may take a while (dataset is ~40GB)")
    
    dataset = load_dataset("openwebtext", split="train", streaming=True)
    
    # We need to materialize the dataset for splitting
    # For streaming, we'll just take a subset for now
    print("Note: Using streaming mode, processing first 10M documents...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Process in chunks
    chunk_size = 100000
    total_docs = 0
    all_tokens = []
    
    for example in tqdm(dataset, desc="Processing"):
        text = example['text']
        tokens = tokenizer.encode(text)
        all_tokens.extend(tokens)
        all_tokens.append(tokenizer.vocab_size - 1)  # EOS
        
        total_docs += 1
        
        # Save chunks periodically
        if len(all_tokens) >= chunk_size * 1000:  # ~100k tokens per chunk
            break
    
    # Split into train/val
    split_idx = int(len(all_tokens) * (1 - val_ratio))
    train_tokens = np.array(all_tokens[:split_idx], dtype=np.uint16)
    val_tokens = np.array(all_tokens[split_idx:], dtype=np.uint16)
    
    # Save
    train_path = os.path.join(output_dir, 'train.bin')
    val_path = os.path.join(output_dir, 'val.bin')
    
    train_tokens.tofile(train_path)
    val_tokens.tofile(val_path)
    
    print(f"\nTrain: {len(train_tokens):,} tokens")
    print(f"Val: {len(val_tokens):,} tokens")
    
    # Save metadata
    metadata = {
        'dataset': 'openwebtext',
        'vocab_size': tokenizer.vocab_size,
        'tokenizer': tokenizer.name,
        'train_tokens': len(train_tokens),
        'val_tokens': len(val_tokens)
    }
    
    with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nDataset preparation complete!")


def prepare_custom(data_path: str, output_dir: str, tokenizer: Tokenizer, val_ratio: float = 0.01):
    """
    Prepare custom dataset from text files.
    
    Args:
        data_path: Path to directory containing text files or a single text file
        output_dir: Output directory for processed data
        tokenizer: Tokenizer to use
        val_ratio: Ratio of data to use for validation
    """
    data_path = Path(data_path)
    
    if data_path.is_file():
        files = [data_path]
    else:
        files = list(data_path.glob('**/*.txt'))
    
    print(f"Found {len(files)} text files")
    
    # Read and tokenize all files
    all_tokens = []
    for file_path in tqdm(files, desc="Processing files"):
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            text = f.read()
        
        tokens = tokenizer.encode(text)
        all_tokens.extend(tokens)
        all_tokens.append(tokenizer.vocab_size - 1)  # EOS
    
    # Split into train/val
    split_idx = int(len(all_tokens) * (1 - val_ratio))
    train_tokens = np.array(all_tokens[:split_idx], dtype=np.uint16)
    val_tokens = np.array(all_tokens[split_idx:], dtype=np.uint16)
    
    # Save
    os.makedirs(output_dir, exist_ok=True)
    
    train_path = os.path.join(output_dir, 'train.bin')
    val_path = os.path.join(output_dir, 'val.bin')
    
    train_tokens.tofile(train_path)
    val_tokens.tofile(val_path)
    
    print(f"\nTrain: {len(train_tokens):,} tokens ({os.path.getsize(train_path) / 1024 / 1024:.2f} MB)")
    print(f"Val: {len(val_tokens):,} tokens ({os.path.getsize(val_path) / 1024 / 1024:.2f} MB)")
    
    # Save metadata
    metadata = {
        'dataset': 'custom',
        'source': str(data_path),
        'vocab_size': tokenizer.vocab_size,
        'tokenizer': tokenizer.name,
        'train_tokens': len(train_tokens),
        'val_tokens': len(val_tokens),
        'num_files': len(files)
    }
    
    with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nDataset preparation complete!")


def main():
    parser = argparse.ArgumentParser(description='Prepare datasets for language model training')
    
    parser.add_argument('--dataset', type=str, required=True,
                        choices=['tinystories', 'openwebtext', 'custom'],
                        help='Dataset to prepare')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='Output directory')
    parser.add_argument('--data_path', type=str, default=None,
                        help='Path to custom data (for custom dataset)')
    parser.add_argument('--tokenizer', type=str, default='gpt2',
                        help='Tokenizer to use')
    parser.add_argument('--val_ratio', type=float, default=0.01,
                        help='Validation split ratio')
    
    args = parser.parse_args()
    
    # Create tokenizer
    tokenizer = Tokenizer(args.tokenizer)
    print(f"Using tokenizer: {tokenizer.name} (vocab_size={tokenizer.vocab_size})")
    
    # Prepare dataset
    if args.dataset == 'tinystories':
        prepare_tinystories(args.output_dir, tokenizer, args.val_ratio)
    elif args.dataset == 'openwebtext':
        prepare_openwebtext(args.output_dir, tokenizer, args.val_ratio)
    elif args.dataset == 'custom':
        if args.data_path is None:
            raise ValueError("--data_path is required for custom dataset")
        prepare_custom(args.data_path, args.output_dir, tokenizer, args.val_ratio)
    
    print(f"\nOutput saved to: {args.output_dir}")


if __name__ == '__main__':
    main()
