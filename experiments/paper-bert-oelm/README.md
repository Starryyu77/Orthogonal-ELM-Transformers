# BERT OELM: Head-wise Orthogonal Initialization for Reservoir Computing

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

> **æ­£äº¤æé™å­¦ä¹ æœº Transformerï¼šåˆ†å¤´æ­£äº¤åˆå§‹åŒ–ä¸å‚¨å±‚è®¡ç®—éªŒè¯**

æœ¬é¡¹ç›®éªŒè¯äº†**åˆ†å¤´æ­£äº¤åˆå§‹åŒ– (Head-wise Orthogonality)** åœ¨ BERT æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜å†»ç»“ Query/Key å‚æ•°ä»…è®­ç»ƒ 87.1% çš„å‚æ•°å³å¯è¾¾åˆ°å…¨å‚æ•°å¾®è°ƒ 98.5% çš„æ€§èƒ½ã€‚

---

## ğŸ¯ æ ¸å¿ƒè´¡çŒ®

1. **åˆ†å¤´æ­£äº¤åˆå§‹åŒ–**: ä¿®å¤äº†å…¨å±€æ­£äº¤å¯¼è‡´çš„æ€§èƒ½å´©å¡Œé—®é¢˜
2. **å‚æ•°é«˜æ•ˆè®­ç»ƒ**: å†»ç»“ 12.9% å‚æ•°ï¼Œä»…æŸå¤± ~1.5% å‡†ç¡®ç‡
3. **è·¨ä»»åŠ¡éªŒè¯**: åœ¨ SST-2 (2åˆ†ç±») å’Œ MNLI (3åˆ†ç±»NLI) ä¸Šå‡æœ‰æ•ˆ
4. **æ­£äº¤æ€§å¿…è¦æ€§**: æ¶ˆèå®éªŒè¯æ˜æ­£äº¤åˆå§‹åŒ–æ˜¯å¿…éœ€çš„

---

## ğŸ“Š ä¸»è¦ç»“æœ

| æ•°æ®é›† | ä»»åŠ¡ | Baseline | OELM-Freeze | å·®è· | OELM è¾¾åˆ°æ¯”ä¾‹ |
|--------|------|----------|-------------|------|---------------|
| **SST-2** | 2åˆ†ç±»æƒ…æ„Ÿåˆ†æ | 93.12% | 91.28% | -1.84% | 98.0% |
| **MNLI** | 3åˆ†ç±»NLI | 83.44% | 82.23% | -1.21% | 98.5% |

**æ¶ˆèå®éªŒ**:
| å®éªŒ | å‡†ç¡®ç‡ | ç»“è®º |
|------|--------|------|
| OELM-Orthogonal | 91.28% | âœ… æ­£äº¤åˆå§‹åŒ–æœ‰æ•ˆ |
| OELM-Random | 82.11% | âŒ éšæœºåˆå§‹åŒ–å¤±è´¥ (-9.17%) |

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
bert-oelm-paper/
â”œâ”€â”€ src/                          # æºä»£ç 
â”‚   â”œâ”€â”€ modeling_bert_oelm.py    # åˆ†å¤´æ­£äº¤åˆå§‹åŒ–å®ç°
â”‚   â”œâ”€â”€ train_bert.py            # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/                      # å®éªŒè„šæœ¬
â”‚   â”œâ”€â”€ run_experiment.sh        # å¿«é€Ÿå®éªŒå¯åŠ¨
â”‚   â””â”€â”€ run_fair_comparison.sh   # å…¬å¹³å¯¹æ¯”å®éªŒ
â”œâ”€â”€ configs/                      # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ sst2_baseline.yaml
â”‚   â”œâ”€â”€ sst2_oelm.yaml
â”‚   â”œâ”€â”€ mnli_baseline.yaml
â”‚   â””â”€â”€ mnli_oelm.yaml
â”œâ”€â”€ experiments/                  # å®éªŒé…ç½®
â”‚   â”œâ”€â”€ sst2/                    # SST-2 å®éªŒé…ç½®
â”‚   â”œâ”€â”€ mnli/                    # MNLI å®éªŒé…ç½®
â”‚   â””â”€â”€ ablation/                # æ¶ˆèå®éªŒé…ç½®
â”œâ”€â”€ results/                      # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ sst2/                    # SST-2 è®­ç»ƒæ—¥å¿—
â”‚   â”œâ”€â”€ mnli/                    # MNLI è®­ç»ƒæ—¥å¿—
â”‚   â”œâ”€â”€ ablation/                # æ¶ˆèå®éªŒæ—¥å¿—
â”‚   â””â”€â”€ timing/                  # è®¡æ—¶åˆ†ææ•°æ®
â”œâ”€â”€ figures/                      # è®ºæ–‡å›¾è¡¨ (å¾…ç”Ÿæˆ)
â”œâ”€â”€ docs/                         # æ–‡æ¡£
â”‚   â””â”€â”€ EXPERIMENT_REPORT.md     # å®Œæ•´å®éªŒæŠ¥å‘Š
â””â”€â”€ README.md                     # æœ¬æ–‡ä»¶
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/bert-oelm.git
cd bert-oelm-paper

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install torch transformers datasets scikit-learn tqdm numpy
```

### è¿è¡Œå®éªŒ

#### SST-2 å®éªŒ

```bash
# Baseline (å…¨å‚æ•°å¾®è°ƒ)
python src/train_bert.py \
    --freeze_mode false \
    --lr 2e-5 \
    --dataset sst2 \
    --output_dir outputs/sst2_baseline

# OELM-Freeze (å†»ç»“ Q/K)
python src/train_bert.py \
    --freeze_mode true \
    --lr 1e-4 \
    --dataset sst2 \
    --init_method orthogonal \
    --output_dir outputs/sst2_oelm
```

#### MNLI å®éªŒ

```bash
# Baseline
python src/train_bert.py \
    --freeze_mode false \
    --lr 2e-5 \
    --dataset mnli \
    --output_dir outputs/mnli_baseline

# OELM-Freeze
python src/train_bert.py \
    --freeze_mode true \
    --lr 1e-4 \
    --dataset mnli \
    --init_method orthogonal \
    --output_dir outputs/mnli_oelm
```

#### æ¶ˆèå®éªŒ (OELM-Random)

```bash
python src/train_bert.py \
    --freeze_mode true \
    --lr 1e-4 \
    --dataset sst2 \
    --init_method normal \
    --output_dir outputs/oelm_random
```

---

## ğŸ”¬ æ ¸å¿ƒç®—æ³•

### åˆ†å¤´æ­£äº¤åˆå§‹åŒ–

```python
def apply_head_wise_orthogonal_(weight: nn.Parameter, num_heads: int) -> None:
    """
    åˆ†å¤´æ­£äº¤åˆå§‹åŒ– - æ ¸å¿ƒåˆ›æ–°

    è¾“å…¥: [hidden_dim, hidden_dim] = [768, 768]
    é‡å¡‘: [num_heads, head_dim, hidden_dim] = [12, 64, 768]
    å¤„ç†: å¯¹æ¯ä¸ª head ç‹¬ç«‹ QR åˆ†è§£
    è¾“å‡º: [hidden_dim, hidden_dim]
    """
    with torch.no_grad():
        hidden_dim = weight.size(0)
        head_dim = hidden_dim // num_heads

        # é‡å¡‘ä¸º [num_heads, head_dim, hidden_dim]
        w = weight.view(num_heads, head_dim, hidden_dim).clone()

        # å¯¹æ¯ä¸ª head ç‹¬ç«‹ QR åˆ†è§£
        for i in range(num_heads):
            q, r = torch.linalg.qr(w[i].T, mode='reduced')
            signs = torch.sign(torch.diag(r))
            q = q * signs.unsqueeze(0)
            w[i] = q.T

        weight.copy_(w.view(hidden_dim, hidden_dim))
```

---

## ğŸ“ˆ å®éªŒå¤ç°

### å…¬å¹³å¯¹æ¯”å®éªŒ

```bash
# è¿è¡Œ 3 è½® AB-AB äº¤å‰éªŒè¯
./scripts/run_fair_comparison.sh 3
```

### å…³é”®å‚æ•°

| å‚æ•° | Baseline | OELM-Freeze | è¯´æ˜ |
|------|----------|-------------|------|
| å†»ç»“ Q/K | âŒ | âœ… | æ ¸å¿ƒåŒºåˆ« |
| å­¦ä¹ ç‡ | 2e-5 | 1e-4 | OELM ä½¿ç”¨æ›´å¤§å­¦ä¹ ç‡ |
| Batch Size | 32 | 32 | ä¿æŒä¸€è‡´ |
| Epochs | 3 | 3 | ä¿æŒä¸€è‡´ |
| Warmup | 10% | 10% | ä¿æŒä¸€è‡´ |

---

## ğŸ“š æ–‡æ¡£

- [å®Œæ•´å®éªŒæŠ¥å‘Š](docs/EXPERIMENT_REPORT_BERT_RESERVOIR.md) - åŒ…å«è¯¦ç»†æ–¹æ³•ã€ç»“æœå’Œè®¨è®º
- [è®­ç»ƒæ—¥å¿—åˆ†æ](results/) - æ‰€æœ‰å®éªŒçš„åŸå§‹æ—¥å¿—
- [è®¡æ—¶åˆ†æ](results/timing/) - å…¬å¹³å¯¹æ¯”å®éªŒçš„è¯¦ç»†è®¡æ—¶æ•°æ®

---

## ğŸ† ä¸»è¦å‘ç°

1. **å‚æ•°æ•ˆç‡**: å†»ç»“ 12.9% å‚æ•°ï¼Œæ€§èƒ½ä»…ä¸‹é™ 1.5%
2. **è®­ç»ƒé€Ÿåº¦**: OELM-Freeze ä¸ Baseline æ— æ˜¾è‘—å·®å¼‚ (+1.4%)
3. **è®­ç»ƒç¨³å®šæ€§**: OELM-Freeze æ›´ç¨³å®š (CV 1.0% vs 9.9%)
4. **æ­£äº¤æ€§å¿…è¦**: OELM-Random æ¯” OELM-Orthogonal ä½ 9.17%
5. **æ³›åŒ–èƒ½åŠ›**: åœ¨å¤æ‚ MNLI ä»»åŠ¡ä¸Šå·®è·ä»… 1.21%

---

## ğŸ”§ ç¯å¢ƒè¦æ±‚

- **Python**: 3.8+
- **PyTorch**: 2.0+
- **Transformers**: 4.x
- **GPU**: NVIDIA GPU with 16GB+ VRAM (æ¨è 24GB)
- **CUDA**: 11.8+

---

## ğŸ“ å¼•ç”¨

å¦‚æœæœ¬å·¥ä½œå¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{zhang2025bertoelm,
  title={BERT OELM: Head-wise Orthogonal Initialization for Efficient Transformer Fine-tuning},
  author={Zhang, Tianyu},
  year={2025},
  institution={NTU MLDA Lab}
}
```

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE) å¼€æºã€‚

---

## ğŸ™ è‡´è°¢

- æŒ‡å¯¼å•ä½: NTU MLDA Lab
- GPU æ”¯æŒ: MLDA GPU Cluster (gpu43.dynip.ntu.edu.sg)
- ä»£ç è¾…åŠ©: Claude Code AI Assistant

---

**æœ€åæ›´æ–°**: 2026-02-08
