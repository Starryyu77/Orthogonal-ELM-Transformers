# MNLI Baseline Configuration
# 自然语言推理全参数微调

experiment:
  name: "mnli_baseline"
  task: "natural_language_inference"
  dataset: "mnli"
  num_classes: 3  # entailment, neutral, contradiction

model:
  name: "bert-base-uncased"
  hidden_size: 768
  num_layers: 12
  num_heads: 12
  intermediate_size: 3072

training:
  freeze_mode: false
  init_method: "normal"
  learning_rate: 2.0e-5
  batch_size: 32
  epochs: 3
  max_steps: -1
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  validate_steps: 500
  max_length: 128

data:
  train_samples: 392702
  val_matched: 9815
  val_mismatched: 9832

results:
  best_accuracy: 83.44%
  best_f1: 0.8885
  validation_runs: 73
  training_time: "~4 hours"
  parameters:
    total: 109.48M
    trainable: 109.48M
    frozen: 0
